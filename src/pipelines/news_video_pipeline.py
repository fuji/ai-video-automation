"""
ãƒ‹ãƒ¥ãƒ¼ã‚¹å‹•ç”»ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³

è¨˜äº‹ã‹ã‚‰è¤‡æ•°ã‚·ãƒ¼ãƒ³ã®ãƒ‹ãƒ¥ãƒ¼ã‚¹å‹•ç”»ã‚’è‡ªå‹•ç”Ÿæˆ
"""

import os
import subprocess
import json
from pathlib import Path
from dataclasses import dataclass, field
from typing import Optional
from datetime import datetime

from PIL import Image, ImageDraw, ImageFont
from rich.console import Console
import google.generativeai as genai

import fal_client
import httpx
import time

from src.generators.image_generator import FluxImageGenerator
from src.config import config
from src.generators.narration_generator import NarrationGenerator
from src.editors.news_graphics import NewsGraphicsCompositor
from src.config import IMAGES_DIR, VIDEOS_DIR, OUTPUT_DIR

# AUDIO_DIR ãŒãªã‘ã‚Œã°ä½œæˆ
AUDIO_DIR = OUTPUT_DIR / "audio"
AUDIO_DIR.mkdir(exist_ok=True)

console = Console()


@dataclass
class Scene:
    """ã‚·ãƒ¼ãƒ³æƒ…å ±"""
    index: int
    description: str  # ã‚·ãƒ¼ãƒ³ã®èª¬æ˜
    image_prompt: str  # Fluxç”¨ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
    video_prompt: str  # Lumaç”¨ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
    subtitle: str  # ã“ã®ã‚·ãƒ¼ãƒ³ã®å­—å¹•
    image_path: Optional[str] = None
    video_path: Optional[str] = None


@dataclass
class NewsVideoResult:
    """ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³çµæœ"""
    success: bool
    video_path: Optional[str] = None
    scenes: list[Scene] = field(default_factory=list)
    audio_path: Optional[str] = None
    duration_seconds: float = 0.0
    error_message: Optional[str] = None


class NewsVideoPipeline:
    """ãƒ‹ãƒ¥ãƒ¼ã‚¹å‹•ç”»ç”Ÿæˆãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³"""
    
    def __init__(
        self,
        channel_name: str = "FJ News 24",
        num_scenes: int = 3,
        scene_duration: float = 5.0,
    ):
        self.channel_name = channel_name
        self.num_scenes = num_scenes
        self.scene_duration = scene_duration
        
        # ã‚¸ã‚§ãƒãƒ¬ãƒ¼ã‚¿ãƒ¼åˆæœŸåŒ–
        self.image_gen = FluxImageGenerator()
        self.narration_gen = NarrationGenerator()
        self.compositor = NewsGraphicsCompositor(channel_name=channel_name)
        
        # Gemini for scene analysis
        genai.configure(api_key=config.gemini.api_key)
        self.gemini = genai.GenerativeModel("gemini-2.0-flash")
        
        # FAL API key for Luma
        os.environ["FAL_KEY"] = config.fal.api_key
        
        console.print(f"[green]NewsVideoPipeline initialized[/green]")
        console.print(f"  Channel: {channel_name}")
        console.print(f"  Scenes: {num_scenes} x {scene_duration}s = {num_scenes * scene_duration}s")
    
    def analyze_article(
        self,
        article_text: str,
        headline: str,
    ) -> list[Scene]:
        """è¨˜äº‹ã‚’åˆ†æã—ã¦è¤‡æ•°ã‚·ãƒ¼ãƒ³ã«åˆ†è§£"""
        
        prompt = f"""ä»¥ä¸‹ã®ãƒ‹ãƒ¥ãƒ¼ã‚¹è¨˜äº‹ã‚’{self.num_scenes}ã¤ã®ã‚·ãƒ¼ãƒ³ã«åˆ†è§£ã—ã¦ãã ã•ã„ã€‚

# è¨˜äº‹
ã‚¿ã‚¤ãƒˆãƒ«: {headline}
æœ¬æ–‡: {article_text}

# å‡ºåŠ›å½¢å¼ (JSON)
å„ã‚·ãƒ¼ãƒ³ã«ã¤ã„ã¦ä»¥ä¸‹ã‚’ç”Ÿæˆ:
- description: ã‚·ãƒ¼ãƒ³ã®èª¬æ˜ï¼ˆæ—¥æœ¬èªã€1æ–‡ï¼‰
- image_prompt: Fluxç”»åƒç”Ÿæˆç”¨ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆï¼ˆè‹±èªã€å…·ä½“çš„ã§è¦–è¦šçš„ãªæå†™ã€50èªä»¥å†…ï¼‰
- video_prompt: Lumaå‹•ç”»ç”Ÿæˆç”¨ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆï¼ˆè‹±èªã€ã‚«ãƒ¡ãƒ©ãƒ¯ãƒ¼ã‚¯ã‚„å‹•ãã®æŒ‡ç¤ºã€20èªä»¥å†…ï¼‰
- subtitle: ã“ã®ã‚·ãƒ¼ãƒ³ã®ãƒŠãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³å­—å¹•ï¼ˆæ—¥æœ¬èªã€15-25æ–‡å­—ï¼‰

# æ³¨æ„
- image_promptã¯ãƒ•ã‚©ãƒˆãƒªã‚¢ãƒªã‚¹ãƒ†ã‚£ãƒƒã‚¯ãªã‚¹ã‚¿ã‚¤ãƒ«ã§
- å„ã‚·ãƒ¼ãƒ³ãŒç‰©èªã¨ã—ã¦ç¹‹ãŒã‚‹ã‚ˆã†ã«
- æœ€åˆã®ã‚·ãƒ¼ãƒ³ã¯çŠ¶æ³èª¬æ˜ã€ä¸­é–“ã¯å±•é–‹ã€æœ€å¾Œã¯çµæœ«

```json
{{
  "scenes": [
    {{
      "description": "...",
      "image_prompt": "...",
      "video_prompt": "...",
      "subtitle": "..."
    }}
  ]
}}
```"""

        console.print("\n[cyan]ğŸ“ è¨˜äº‹ã‚’åˆ†æä¸­...[/cyan]")
        
        response = self.gemini.generate_content(prompt)
        
        # JSONã‚’æŠ½å‡º
        content = response.text
        json_start = content.find("{")
        json_end = content.rfind("}") + 1
        json_str = content[json_start:json_end]
        
        data = json.loads(json_str)
        
        scenes = []
        for i, scene_data in enumerate(data["scenes"]):
            scene = Scene(
                index=i,
                description=scene_data["description"],
                image_prompt=scene_data["image_prompt"],
                video_prompt=scene_data["video_prompt"],
                subtitle=scene_data["subtitle"],
            )
            scenes.append(scene)
            console.print(f"  ã‚·ãƒ¼ãƒ³{i+1}: {scene.description}")
        
        return scenes
    
    def generate_scene_images(
        self,
        scenes: list[Scene],
        output_prefix: str,
    ) -> list[Scene]:
        """å„ã‚·ãƒ¼ãƒ³ã®ç”»åƒã‚’ç”Ÿæˆ"""
        
        console.print("\n[cyan]ğŸ–¼ï¸ ã‚·ãƒ¼ãƒ³ç”»åƒã‚’ç”Ÿæˆä¸­...[/cyan]")
        
        for scene in scenes:
            output_name = f"{output_prefix}_scene{scene.index + 1}"
            
            result = self.image_gen.generate(
                prompt=scene.image_prompt,
                output_name=output_name,
                image_size="portrait_16_9",  # ç¸¦å‹•ç”»ç”¨
            )
            
            if result.success:
                scene.image_path = result.file_path
                console.print(f"  âœ… ã‚·ãƒ¼ãƒ³{scene.index + 1}: {result.file_path}")
            else:
                console.print(f"  âŒ ã‚·ãƒ¼ãƒ³{scene.index + 1}: {result.error_message}")
        
        return scenes
    
    def generate_scene_videos(
        self,
        scenes: list[Scene],
        output_prefix: str,
    ) -> list[Scene]:
        """å„ã‚·ãƒ¼ãƒ³ã®å‹•ç”»ã‚’ç”Ÿæˆï¼ˆLuma Dream Machine via fal.aiï¼‰"""
        
        console.print("\n[cyan]ğŸ¬ ã‚·ãƒ¼ãƒ³å‹•ç”»ã‚’ç”Ÿæˆä¸­ (Luma)...[/cyan]")
        
        for scene in scenes:
            if not scene.image_path:
                console.print(f"  âš ï¸ ã‚·ãƒ¼ãƒ³{scene.index + 1}: ç”»åƒãŒã‚ã‚Šã¾ã›ã‚“")
                continue
            
            output_path = str(VIDEOS_DIR / f"{output_prefix}_scene{scene.index + 1}.mp4")
            
            try:
                # ç”»åƒã‚’fal.aiã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
                image_url = fal_client.upload_file(scene.image_path)
                console.print(f"  ğŸ“¤ ã‚·ãƒ¼ãƒ³{scene.index + 1}: ç”»åƒã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰å®Œäº†")
                
                # Luma APIå‘¼ã³å‡ºã—
                result = fal_client.subscribe(
                    "fal-ai/luma-dream-machine/image-to-video",
                    arguments={
                        "prompt": scene.video_prompt,
                        "image_url": image_url,
                        "aspect_ratio": "9:16",
                    },
                    with_logs=False,
                )
                
                # å‹•ç”»ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
                video_url = result["video"]["url"]
                response = httpx.get(video_url)
                with open(output_path, "wb") as f:
                    f.write(response.content)
                
                scene.video_path = output_path
                console.print(f"  âœ… ã‚·ãƒ¼ãƒ³{scene.index + 1}: {output_path}")
                
            except Exception as e:
                console.print(f"  âŒ ã‚·ãƒ¼ãƒ³{scene.index + 1}: {str(e)}")
        
        return scenes
    
    def generate_narration(
        self,
        scenes: list[Scene],
        output_prefix: str,
    ) -> str:
        """ã‚·ãƒ¼ãƒ³ã®å­—å¹•ã‹ã‚‰ãƒŠãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³éŸ³å£°ã‚’ç”Ÿæˆ"""
        
        console.print("\n[cyan]ğŸ¤ ãƒŠãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ç”Ÿæˆä¸­...[/cyan]")
        
        # å…¨ã‚·ãƒ¼ãƒ³ã®å­—å¹•ã‚’çµåˆ
        full_text = "ã€‚".join([scene.subtitle for scene in scenes]) + "ã€‚"
        
        output_path = str(AUDIO_DIR / f"{output_prefix}_narration.mp3")
        
        result = self.narration_gen.generate(
            text=full_text,
            output_path=output_path,
        )
        
        if result.success:
            console.print(f"  âœ… éŸ³å£°: {result.file_path} ({result.duration_seconds:.1f}ç§’)")
            return result.file_path
        else:
            console.print(f"  âŒ éŸ³å£°ç”Ÿæˆå¤±æ•—: {result.error_message}")
            return None
    
    def compose_final_video(
        self,
        scenes: list[Scene],
        audio_path: str,
        headline: str,
        sub_headline: str,
        output_prefix: str,
        is_breaking: bool = True,
    ) -> str:
        """å…¨ã‚·ãƒ¼ãƒ³ã‚’çµåˆã—ã¦æœ€çµ‚å‹•ç”»ã‚’ä½œæˆ"""
        
        console.print("\n[cyan]ğŸ¬ æœ€çµ‚å‹•ç”»ã‚’åˆæˆä¸­...[/cyan]")
        
        # å‹•ç”»ãŒã‚ã‚‹ã‚·ãƒ¼ãƒ³ã ã‘æŠ½å‡º
        valid_scenes = [s for s in scenes if s.video_path]
        if not valid_scenes:
            raise ValueError("æœ‰åŠ¹ãªå‹•ç”»ãŒã‚ã‚Šã¾ã›ã‚“")
        
        # æœ€åˆã®å‹•ç”»ã‹ã‚‰ã‚µã‚¤ã‚ºã‚’å–å¾—
        probe = subprocess.run(
            ["ffprobe", "-v", "error", "-select_streams", "v:0",
             "-show_entries", "stream=width,height", "-of", "csv=p=0",
             valid_scenes[0].video_path],
            capture_output=True, text=True
        )
        width, height = map(int, probe.stdout.strip().split(','))
        
        # ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ç”¨ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
        temp_dir = VIDEOS_DIR / "temp"
        temp_dir.mkdir(exist_ok=True)
        
        # 1. å„ã‚·ãƒ¼ãƒ³ã«ã‚ªãƒ¼ãƒãƒ¼ãƒ¬ã‚¤ã¨å­—å¹•ã‚’è¿½åŠ 
        overlaid_videos = []
        
        for scene in valid_scenes:
            # ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚ªãƒ¼ãƒãƒ¼ãƒ¬ã‚¤ä½œæˆ
            overlay_path = str(temp_dir / f"overlay_{scene.index}.png")
            self.compositor.create_transparent_overlay(
                width=width, height=height,
                headline=headline,
                sub_headline=sub_headline,
                is_breaking=is_breaking,
                style="solid",
                output_path=overlay_path,
            )
            
            # å­—å¹•ã‚’è¿½åŠ 
            overlay_img = Image.open(overlay_path).convert("RGBA")
            draw = ImageDraw.Draw(overlay_img)
            
            font = ImageFont.truetype(
                "/System/Library/Fonts/ãƒ’ãƒ©ã‚®ãƒè§’ã‚´ã‚·ãƒƒã‚¯ W3.ttc",
                int(height * 0.032)
            )
            
            # å­—å¹•ã‚’è¤‡æ•°è¡Œã«åˆ†å‰²ï¼ˆé•·ã„å ´åˆï¼‰
            subtitle = scene.subtitle
            if len(subtitle) > 15:
                mid = len(subtitle) // 2
                for i in range(mid, 0, -1):
                    if subtitle[i] in 'ãŒã®ã‚’ã«ã¯ã§ã¨ã€ã€‚':
                        mid = i + 1
                        break
                lines = [subtitle[:mid], subtitle[mid:]]
            else:
                lines = [subtitle]
            
            margin_x = int(width * 0.10)
            max_text_width = width - margin_x * 2
            line_height = int(height * 0.045)
            total_text_height = len(lines) * line_height
            start_y = (height - total_text_height) // 2
            
            for i, line in enumerate(lines):
                bbox = draw.textbbox((0, 0), line, font=font)
                text_w = bbox[2] - bbox[0]
                text_x = margin_x + (max_text_width - text_w) // 2
                y = start_y + i * line_height
                draw.text(
                    (text_x, y), line, font=font,
                    fill=(255, 255, 255, 255),
                    stroke_width=3,
                    stroke_fill=(0, 0, 0, 255)
                )
            
            scene_overlay_path = str(temp_dir / f"scene_overlay_{scene.index}.png")
            overlay_img.save(scene_overlay_path, "PNG")
            
            # FFmpegã§ã‚ªãƒ¼ãƒãƒ¼ãƒ¬ã‚¤åˆæˆ
            overlaid_path = str(temp_dir / f"overlaid_{scene.index}.mp4")
            subprocess.run([
                "ffmpeg", "-y",
                "-i", scene.video_path,
                "-i", scene_overlay_path,
                "-filter_complex", "[0:v][1:v]overlay=0:0",
                "-c:v", "libx264", "-preset", "fast", "-crf", "18",
                "-an", overlaid_path
            ], capture_output=True)
            
            overlaid_videos.append(overlaid_path)
            console.print(f"  âœ… ã‚·ãƒ¼ãƒ³{scene.index + 1} ã‚ªãƒ¼ãƒãƒ¼ãƒ¬ã‚¤é©ç”¨")
        
        # 2. å‹•ç”»ã‚’çµåˆ
        concat_list_path = str(temp_dir / "concat_list.txt")
        with open(concat_list_path, "w") as f:
            for video_path in overlaid_videos:
                f.write(f"file '{video_path}'\n")
        
        concat_video_path = str(temp_dir / "concat.mp4")
        subprocess.run([
            "ffmpeg", "-y",
            "-f", "concat", "-safe", "0",
            "-i", concat_list_path,
            "-c", "copy",
            concat_video_path
        ], capture_output=True)
        console.print("  âœ… å‹•ç”»çµåˆå®Œäº†")
        
        # 3. éŸ³å£°ã‚’è¿½åŠ 
        final_path = str(VIDEOS_DIR / f"{output_prefix}_final.mp4")
        subprocess.run([
            "ffmpeg", "-y",
            "-i", concat_video_path,
            "-i", audio_path,
            "-c:v", "copy",
            "-c:a", "aac", "-b:a", "192k",
            "-shortest",
            final_path
        ], capture_output=True)
        
        console.print(f"\n[green]ğŸ‰ å®Œæˆ: {final_path}[/green]")
        
        return final_path
    
    def run(
        self,
        article_text: str,
        headline: str,
        sub_headline: str = "",
        output_prefix: Optional[str] = None,
        is_breaking: bool = True,
    ) -> NewsVideoResult:
        """ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å…¨ä½“ã‚’å®Ÿè¡Œ"""
        
        if output_prefix is None:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            output_prefix = f"news_{timestamp}"
        
        console.print("\n" + "=" * 50)
        console.print(f"[bold]ğŸ“° ãƒ‹ãƒ¥ãƒ¼ã‚¹å‹•ç”»ç”Ÿæˆ: {headline[:30]}...[/bold]")
        console.print("=" * 50)
        
        try:
            # 1. è¨˜äº‹åˆ†æ
            scenes = self.analyze_article(article_text, headline)
            
            # 2. ç”»åƒç”Ÿæˆ
            scenes = self.generate_scene_images(scenes, output_prefix)
            
            # 3. å‹•ç”»ç”Ÿæˆ
            scenes = self.generate_scene_videos(scenes, output_prefix)
            
            # 4. ãƒŠãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ç”Ÿæˆ
            audio_path = self.generate_narration(scenes, output_prefix)
            
            # 5. æœ€çµ‚åˆæˆ
            final_path = self.compose_final_video(
                scenes=scenes,
                audio_path=audio_path,
                headline=headline,
                sub_headline=sub_headline,
                output_prefix=output_prefix,
                is_breaking=is_breaking,
            )
            
            # å‹•ç”»ã®é•·ã•ã‚’å–å¾—
            probe = subprocess.run(
                ["ffprobe", "-v", "error", "-show_entries", "format=duration",
                 "-of", "default=noprint_wrappers=1:nokey=1", final_path],
                capture_output=True, text=True
            )
            duration = float(probe.stdout.strip())
            
            return NewsVideoResult(
                success=True,
                video_path=final_path,
                scenes=scenes,
                audio_path=audio_path,
                duration_seconds=duration,
            )
            
        except Exception as e:
            console.print(f"[red]âŒ ã‚¨ãƒ©ãƒ¼: {e}[/red]")
            import traceback
            traceback.print_exc()
            return NewsVideoResult(
                success=False,
                error_message=str(e),
            )


# CLIç”¨
if __name__ == "__main__":
    import sys
    
    pipeline = NewsVideoPipeline()
    
    # ãƒ†ã‚¹ãƒˆç”¨
    result = pipeline.run(
        article_text="""
        ã‚¹ãƒšã‚¤ãƒ³ã§è¡Œæ–¹ä¸æ˜ã«ãªã£ãŸçŒ«ãŒã€5ãƒ¶æœˆã‹ã‘ã¦250ã‚­ãƒ­ã‚’æ­©ãã€
        ãƒ•ãƒ©ãƒ³ã‚¹ã®è‡ªå®…ã«å¸°é‚„ã—ã¾ã—ãŸã€‚é£¼ã„ä¸»ã®ãƒ•ã‚¡ãƒ“ã‚¢ãƒ³ã•ã‚“ã¯ã€
        æ„›çŒ«ãƒŸãƒŒã‚·ãƒ¥ãŒæˆ»ã£ã¦ããŸæ™‚ã€ä¿¡ã˜ã‚‰ã‚Œãªã‹ã£ãŸã¨èªã£ã¦ã„ã¾ã™ã€‚
        çŒ«ã¯å°‘ã—ç—©ã›ã¦ã„ã¾ã—ãŸãŒã€å…ƒæ°—ãªæ§˜å­ã§ã—ãŸã€‚
        """,
        headline="çŒ«ãŒ250kmæ­©ã„ã¦ã‚¹ãƒšã‚¤ãƒ³ã‹ã‚‰ãƒ•ãƒ©ãƒ³ã‚¹ã®è‡ªå®…ã«å¸°é‚„",
        sub_headline="5ãƒ¶æœˆã‹ã‘ã¦155ãƒã‚¤ãƒ«ã‚’è¸ç ´",
        output_prefix="cat_journey",
    )
    
    print(f"\nçµæœ: {'æˆåŠŸ' if result.success else 'å¤±æ•—'}")
    if result.success:
        print(f"å‹•ç”»: {result.video_path}")
        print(f"é•·ã•: {result.duration_seconds:.1f}ç§’")
