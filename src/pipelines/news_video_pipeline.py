"""
ãƒ‹ãƒ¥ãƒ¼ã‚¹å‹•ç”»ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³

è¨˜äº‹ã‹ã‚‰è¤‡æ•°ã‚·ãƒ¼ãƒ³ã®ãƒ‹ãƒ¥ãƒ¼ã‚¹å‹•ç”»ã‚’è‡ªå‹•ç”Ÿæˆ
"""

import os
import subprocess
import json
from pathlib import Path
from dataclasses import dataclass, field
from typing import Optional
from datetime import datetime

from PIL import Image, ImageDraw, ImageFont
from rich.console import Console
from google import genai

import fal_client
import httpx
import time

from src.generators.image_generator import FluxImageGenerator
from src.config import config, get_daily_output_dirs
from src.generators.edge_tts_generator import EdgeTTSGenerator  # ç„¡æ–™TTS
from src.editors.news_graphics import NewsGraphicsCompositor
from src.audio.bgm_manager import BGMManager, MoodType

console = Console()


@dataclass
class Scene:
    """ã‚·ãƒ¼ãƒ³æƒ…å ±"""
    index: int
    description: str  # ã‚·ãƒ¼ãƒ³ã®èª¬æ˜
    image_prompt: str  # Fluxç”¨ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
    video_prompt: str  # Lumaç”¨ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
    subtitle: str  # ã“ã®ã‚·ãƒ¼ãƒ³ã®å­—å¹•
    image_path: Optional[str] = None
    video_path: Optional[str] = None


@dataclass
class NewsVideoResult:
    """ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³çµæœ"""
    success: bool
    video_path: Optional[str] = None
    scenes: list[Scene] = field(default_factory=list)
    audio_path: Optional[str] = None
    duration_seconds: float = 0.0
    error_message: Optional[str] = None


class NewsVideoPipeline:
    """ãƒ‹ãƒ¥ãƒ¼ã‚¹å‹•ç”»ç”Ÿæˆãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³"""
    
    def __init__(
        self,
        channel_name: str = "FJ News 24",
        num_scenes: int = 4,  # 4ã‚·ãƒ¼ãƒ³ã§ç´„20ç§’ã®ãƒ™ãƒ¼ã‚¹å‹•ç”»
        scene_duration: float = 5.0,
    ):
        self.channel_name = channel_name
        self.num_scenes = num_scenes
        self.scene_duration = scene_duration
        
        # æ—¥ä»˜ãƒ™ãƒ¼ã‚¹ã®å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
        self.dirs = get_daily_output_dirs()
        
        # ã‚¸ã‚§ãƒãƒ¬ãƒ¼ã‚¿ãƒ¼åˆæœŸåŒ–
        self.image_gen = FluxImageGenerator()
        self.narration_gen = EdgeTTSGenerator()  # ç„¡æ–™TTS (Edge TTS)
        self.compositor = NewsGraphicsCompositor(channel_name=channel_name)
        self.bgm_manager = BGMManager()  # BGMç®¡ç†
        
        # Gemini for scene analysis
        self.gemini_client = genai.Client(api_key=config.gemini.api_key)
        
        # FAL API key for Luma
        os.environ["FAL_KEY"] = config.fal.api_key
        
        console.print(f"[green]NewsVideoPipeline initialized[/green]")
        console.print(f"  Output: {self.dirs['root']}")
        console.print(f"  Channel: {channel_name}")
        console.print(f"  Scenes: {num_scenes} x {scene_duration}s = {num_scenes * scene_duration}s")
    
    def analyze_article(
        self,
        article_text: str,
        headline: str,
    ) -> list[Scene]:
        """è¨˜äº‹ã‚’åˆ†æã—ã¦è¤‡æ•°ã‚·ãƒ¼ãƒ³ã«åˆ†è§£"""
        
        prompt = f"""ä»¥ä¸‹ã®ãƒ‹ãƒ¥ãƒ¼ã‚¹è¨˜äº‹ã‚’{self.num_scenes}ã¤ã®æ˜ åƒçš„ãªã‚·ãƒ¼ãƒ³ã«åˆ†è§£ã—ã¦ãã ã•ã„ã€‚

# è¨˜äº‹
ã‚¿ã‚¤ãƒˆãƒ«: {headline}
æœ¬æ–‡: {article_text}

# ã‚·ãƒ¼ãƒ³æ§‹æˆã‚¬ã‚¤ãƒ‰ï¼ˆ{self.num_scenes}ã‚·ãƒ¼ãƒ³ï¼‰
1. ã‚ªãƒ¼ãƒ—ãƒ‹ãƒ³ã‚°: çŠ¶æ³è¨­å®šã€ä¸»äººå…¬ã‚„èˆå°ã®ç´¹ä»‹
2. å±•é–‹1: å‡ºæ¥äº‹ã®å§‹ã¾ã‚Šã€å•é¡Œã‚„çŠ¶æ³ã®ç™ºç”Ÿ
3. å±•é–‹2: ã‚¯ãƒ©ã‚¤ãƒãƒƒã‚¯ã‚¹ã€æœ€ã‚‚å°è±¡çš„ãªç¬é–“
4. ã‚¨ãƒ³ãƒ‡ã‚£ãƒ³ã‚°: çµæœ«ã€ç¾åœ¨ã®çŠ¶æ³ã€ä½™éŸ»

# å‡ºåŠ›å½¢å¼ (JSON)
å„ã‚·ãƒ¼ãƒ³ã«ã¤ã„ã¦ä»¥ä¸‹ã‚’ç”Ÿæˆ:
- description: ã‚·ãƒ¼ãƒ³ã®èª¬æ˜ï¼ˆæ—¥æœ¬èªã€1æ–‡ã§æ˜ åƒã‚’ã‚¤ãƒ¡ãƒ¼ã‚¸ã§ãã‚‹ã‚ˆã†ã«ï¼‰
- image_prompt: Fluxç”»åƒç”Ÿæˆç”¨ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆï¼ˆè‹±èªã€70èªä»¥å†…ï¼‰
  * å…·ä½“çš„ãªè¢«å†™ä½“ã€å ´æ‰€ã€æ™‚é–“å¸¯ã€é›°å›²æ°—ã‚’å«ã‚ã‚‹
  * "photorealistic, cinematic lighting, 4K quality" ã‚’å«ã‚ã‚‹
  * äººç‰©ãŒã„ã‚‹å ´åˆã¯è¡¨æƒ…ã‚„å‹•ä½œã‚‚æå†™
- video_prompt: Lumaå‹•ç”»ç”Ÿæˆç”¨ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆï¼ˆè‹±èªã€25èªä»¥å†…ï¼‰
  * ã‚«ãƒ¡ãƒ©ãƒ¯ãƒ¼ã‚¯ï¼ˆpan, zoom, dollyç­‰ï¼‰ã‚’æŒ‡å®š
  * å‹•ãã®æ–¹å‘ã¨é€Ÿåº¦ã‚’å«ã‚ã‚‹
- subtitle: ã“ã®ã‚·ãƒ¼ãƒ³ã®å­—å¹•ï¼ˆæ—¥æœ¬èªã€20-30æ–‡å­—ã€æ„Ÿæƒ…ãŒä¼ã‚ã‚‹ã‚ˆã†ã«ï¼‰

```json
{{
  "scenes": [
    {{
      "description": "...",
      "image_prompt": "...",
      "video_prompt": "...",
      "subtitle": "..."
    }}
  ]
}}
```"""

        console.print("\n[cyan]ğŸ“ è¨˜äº‹ã‚’åˆ†æä¸­...[/cyan]")
        
        response = self.gemini_client.models.generate_content(
            model="gemini-2.0-flash",
            contents=prompt,
        )
        
        # JSONã‚’æŠ½å‡º
        content = response.text
        json_start = content.find("{")
        json_end = content.rfind("}") + 1
        json_str = content[json_start:json_end]
        
        data = json.loads(json_str)
        
        scenes = []
        for i, scene_data in enumerate(data["scenes"]):
            scene = Scene(
                index=i,
                description=scene_data["description"],
                image_prompt=scene_data["image_prompt"],
                video_prompt=scene_data["video_prompt"],
                subtitle=scene_data["subtitle"],
            )
            scenes.append(scene)
            console.print(f"  ã‚·ãƒ¼ãƒ³{i+1}: {scene.description}")
        
        return scenes
    
    def generate_scene_images(
        self,
        scenes: list[Scene],
        output_prefix: str,
    ) -> list[Scene]:
        """å„ã‚·ãƒ¼ãƒ³ã®ç”»åƒã‚’ç”Ÿæˆ"""
        
        console.print("\n[cyan]ğŸ–¼ï¸ ã‚·ãƒ¼ãƒ³ç”»åƒã‚’ç”Ÿæˆä¸­...[/cyan]")
        
        for scene in scenes:
            output_name = f"{output_prefix}_scene{scene.index + 1}"
            
            result = self.image_gen.generate(
                prompt=scene.image_prompt,
                output_name=output_name,
                image_size="portrait_16_9",  # ç¸¦å‹•ç”»ç”¨
                output_dir=self.dirs["images"],
            )
            
            if result.success:
                scene.image_path = result.file_path
                console.print(f"  âœ… ã‚·ãƒ¼ãƒ³{scene.index + 1}: {result.file_path}")
            else:
                console.print(f"  âŒ ã‚·ãƒ¼ãƒ³{scene.index + 1}: {result.error_message}")
        
        return scenes
    
    def generate_scene_videos(
        self,
        scenes: list[Scene],
        output_prefix: str,
    ) -> list[Scene]:
        """å„ã‚·ãƒ¼ãƒ³ã®å‹•ç”»ã‚’ç”Ÿæˆï¼ˆLuma Dream Machine via fal.aiï¼‰"""
        
        console.print("\n[cyan]ğŸ¬ ã‚·ãƒ¼ãƒ³å‹•ç”»ã‚’ç”Ÿæˆä¸­ (Luma)...[/cyan]")
        
        for scene in scenes:
            if not scene.image_path:
                console.print(f"  âš ï¸ ã‚·ãƒ¼ãƒ³{scene.index + 1}: ç”»åƒãŒã‚ã‚Šã¾ã›ã‚“")
                continue
            
            output_path = str(self.dirs["videos"] / f"{output_prefix}_scene{scene.index + 1}.mp4")
            
            try:
                # ç”»åƒã‚’fal.aiã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
                image_url = fal_client.upload_file(scene.image_path)
                console.print(f"  ğŸ“¤ ã‚·ãƒ¼ãƒ³{scene.index + 1}: ç”»åƒã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰å®Œäº†")
                
                # Luma APIå‘¼ã³å‡ºã—
                result = fal_client.subscribe(
                    "fal-ai/luma-dream-machine/image-to-video",
                    arguments={
                        "prompt": scene.video_prompt,
                        "image_url": image_url,
                        "aspect_ratio": "9:16",
                    },
                    with_logs=False,
                )
                
                # å‹•ç”»ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
                video_url = result["video"]["url"]
                response = httpx.get(video_url)
                with open(output_path, "wb") as f:
                    f.write(response.content)
                
                scene.video_path = output_path
                console.print(f"  âœ… ã‚·ãƒ¼ãƒ³{scene.index + 1}: {output_path}")
                
            except Exception as e:
                console.print(f"  âŒ ã‚·ãƒ¼ãƒ³{scene.index + 1}: {str(e)}")
        
        return scenes
    
    def generate_narration(
        self,
        article_text: str,
        output_prefix: str,
        closing_text: str = "",
    ) -> tuple[str, float]:
        """è¨˜äº‹å…¨æ–‡ã‹ã‚‰ãƒŠãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³éŸ³å£°ã‚’ç”Ÿæˆï¼ˆç· ã‚ãƒŠãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³å«ã‚€ï¼‰
        
        Args:
            article_text: ãƒŠãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ç”¨ã®è¨˜äº‹ãƒ†ã‚­ã‚¹ãƒˆ
            output_prefix: å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«åãƒ—ãƒ¬ãƒ•ã‚£ãƒƒã‚¯ã‚¹
            closing_text: ç· ã‚ãƒŠãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ï¼ˆçœç•¥å¯ï¼‰
        
        Returns:
            tuple: (audio_path, total_duration)
        """
        
        console.print("\n[cyan]ğŸ¤ ãƒŠãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ç”Ÿæˆä¸­...[/cyan]")
        
        # è¨˜äº‹å…¨æ–‡ã‚’ãƒŠãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã«ä½¿ç”¨
        full_text = article_text
        
        main_path = str(self.dirs["audio"] / f"{output_prefix}_narration.mp3")
        result = self.narration_gen.generate(text=full_text, output_path=main_path)
        
        if not result.success:
            console.print(f"  âŒ éŸ³å£°ç”Ÿæˆå¤±æ•—: {result.error_message}")
            return None, 0
        
        console.print(f"  âœ… æœ¬ç·¨éŸ³å£°: {result.file_path} ({result.duration_seconds:.1f}ç§’)")
        
        # ç· ã‚ãƒŠãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ãŒã‚ã‚Œã°è¿½åŠ 
        if closing_text:
            console.print("  ğŸ¤ ç· ã‚ãƒŠãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ç”Ÿæˆä¸­...")
            closing_path = str(self.dirs["audio"] / f"{output_prefix}_closing.mp3")
            closing_result = self.narration_gen.generate(text=closing_text, output_path=closing_path)
            
            if closing_result.success:
                console.print(f"  âœ… ç· ã‚éŸ³å£°: {closing_result.file_path} ({closing_result.duration_seconds:.1f}ç§’)")
                
                # éŸ³å£°ã‚’çµåˆ
                combined_path = str(self.dirs["audio"] / f"{output_prefix}_full.mp3")
                subprocess.run([
                    "ffmpeg", "-y",
                    "-i", main_path, "-i", closing_path,
                    "-filter_complex", "[0:a][1:a]concat=n=2:v=0:a=1[a]",
                    "-map", "[a]", combined_path
                ], capture_output=True)
                
                # çµåˆå¾Œã®é•·ã•ã‚’å–å¾—
                probe = subprocess.run(
                    ["ffprobe", "-v", "error", "-show_entries", "format=duration",
                     "-of", "default=noprint_wrappers=1:nokey=1", combined_path],
                    capture_output=True, text=True
                )
                total_duration = float(probe.stdout.strip())
                console.print(f"  âœ… åˆè¨ˆéŸ³å£°: {total_duration:.1f}ç§’")
                return combined_path, total_duration
        
        return result.file_path, result.duration_seconds
    
    def compose_final_video(
        self,
        scenes: list[Scene],
        audio_path: str,
        audio_duration: float,
        headline: str,
        sub_headline: str,
        output_prefix: str,
        is_breaking: bool = True,
    ) -> str:
        """å…¨ã‚·ãƒ¼ãƒ³ã‚’çµåˆã—ã¦æœ€çµ‚å‹•ç”»ã‚’ä½œæˆï¼ˆéŸ³å£°é•·ã«åˆã‚ã›ã¦ã‚¹ãƒ­ãƒ¼èª¿æ•´ï¼‰"""
        
        console.print("\n[cyan]ğŸ¬ æœ€çµ‚å‹•ç”»ã‚’åˆæˆä¸­...[/cyan]")
        
        # å‹•ç”»ãŒã‚ã‚‹ã‚·ãƒ¼ãƒ³ã ã‘æŠ½å‡º
        valid_scenes = [s for s in scenes if s.video_path]
        if not valid_scenes:
            raise ValueError("æœ‰åŠ¹ãªå‹•ç”»ãŒã‚ã‚Šã¾ã›ã‚“")
        
        # æœ€åˆã®å‹•ç”»ã‹ã‚‰ã‚µã‚¤ã‚ºã‚’å–å¾—
        probe = subprocess.run(
            ["ffprobe", "-v", "error", "-select_streams", "v:0",
             "-show_entries", "stream=width,height", "-of", "csv=p=0",
             valid_scenes[0].video_path],
            capture_output=True, text=True
        )
        width, height = map(int, probe.stdout.strip().split(','))
        
        # ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ç”¨ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
        temp_dir = self.dirs["temp"]
        
        # 1. å„ã‚·ãƒ¼ãƒ³ã«ã‚ªãƒ¼ãƒãƒ¼ãƒ¬ã‚¤ã¨å­—å¹•ã‚’è¿½åŠ 
        overlaid_videos = []
        
        for scene in valid_scenes:
            # ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚ªãƒ¼ãƒãƒ¼ãƒ¬ã‚¤ä½œæˆ
            overlay_path = str(temp_dir / f"overlay_{scene.index}.png")
            self.compositor.create_transparent_overlay(
                width=width, height=height,
                headline=headline,
                sub_headline=sub_headline,
                is_breaking=is_breaking,
                style="solid",
                output_path=overlay_path,
            )
            
            # å­—å¹•ã‚’è¿½åŠ 
            overlay_img = Image.open(overlay_path).convert("RGBA")
            draw = ImageDraw.Draw(overlay_img)
            
            font = ImageFont.truetype(
                "/System/Library/Fonts/ãƒ’ãƒ©ã‚®ãƒè§’ã‚´ã‚·ãƒƒã‚¯ W3.ttc",
                int(height * 0.032)
            )
            
            # å­—å¹•ã‚’è¤‡æ•°è¡Œã«åˆ†å‰²ï¼ˆé•·ã„å ´åˆï¼‰
            subtitle = scene.subtitle
            if len(subtitle) > 15:
                mid = len(subtitle) // 2
                for i in range(mid, 0, -1):
                    if subtitle[i] in 'ãŒã®ã‚’ã«ã¯ã§ã¨ã€ã€‚':
                        mid = i + 1
                        break
                lines = [subtitle[:mid], subtitle[mid:]]
            else:
                lines = [subtitle]
            
            margin_x = int(width * 0.10)
            max_text_width = width - margin_x * 2
            line_height = int(height * 0.045)
            total_text_height = len(lines) * line_height
            start_y = (height - total_text_height) // 2
            
            for i, line in enumerate(lines):
                bbox = draw.textbbox((0, 0), line, font=font)
                text_w = bbox[2] - bbox[0]
                text_x = margin_x + (max_text_width - text_w) // 2
                y = start_y + i * line_height
                draw.text(
                    (text_x, y), line, font=font,
                    fill=(255, 255, 255, 255),
                    stroke_width=3,
                    stroke_fill=(0, 0, 0, 255)
                )
            
            scene_overlay_path = str(temp_dir / f"scene_overlay_{scene.index}.png")
            overlay_img.save(scene_overlay_path, "PNG")
            
            # FFmpegã§ã‚ªãƒ¼ãƒãƒ¼ãƒ¬ã‚¤åˆæˆ
            overlaid_path = str(temp_dir / f"overlaid_{scene.index}.mp4")
            subprocess.run([
                "ffmpeg", "-y",
                "-i", scene.video_path,
                "-i", scene_overlay_path,
                "-filter_complex", "[0:v][1:v]overlay=0:0",
                "-c:v", "libx264", "-preset", "fast", "-crf", "18",
                "-an", overlaid_path
            ], capture_output=True)
            
            overlaid_videos.append(overlaid_path)
            console.print(f"  âœ… ã‚·ãƒ¼ãƒ³{scene.index + 1} ã‚ªãƒ¼ãƒãƒ¼ãƒ¬ã‚¤é©ç”¨")
        
        # 2. å„ã‚·ãƒ¼ãƒ³ã®é•·ã•ã‚’å–å¾—
        def get_duration(path):
            probe = subprocess.run(
                ["ffprobe", "-v", "error", "-show_entries", "format=duration",
                 "-of", "default=noprint_wrappers=1:nokey=1", path],
                capture_output=True, text=True
            )
            return float(probe.stdout.strip())
        
        video_durations = [get_duration(v) for v in overlaid_videos]
        total_video_duration = sum(video_durations)
        
        console.print(f"  å‹•ç”»åˆè¨ˆ: {total_video_duration:.1f}ç§’, éŸ³å£°: {audio_duration:.1f}ç§’")
        
        # 3. éŸ³å£°ãŒé•·ã„å ´åˆã€æœ€å¾Œã®ã‚·ãƒ¼ãƒ³ã‚’ã‚¹ãƒ­ãƒ¼ã«ã—ã¦èª¿æ•´
        if audio_duration > total_video_duration:
            other_scenes_duration = sum(video_durations[:-1])
            needed_last_scene = audio_duration - other_scenes_duration + 0.3
            slowdown_factor = needed_last_scene / video_durations[-1]
            
            console.print(f"  æœ€å¾Œã®ã‚·ãƒ¼ãƒ³ã‚’ {slowdown_factor:.2f}x ã‚¹ãƒ­ãƒ¼ã«èª¿æ•´")
            
            # æœ€å¾Œã®ã‚·ãƒ¼ãƒ³ã‚’ã‚¹ãƒ­ãƒ¼åŒ–
            last_scene_slow = str(temp_dir / "last_scene_slow.mp4")
            subprocess.run([
                "ffmpeg", "-y", "-i", overlaid_videos[-1],
                "-filter:v", f"setpts={slowdown_factor}*PTS",
                "-c:v", "libx264", "-preset", "fast", "-crf", "18",
                "-an", last_scene_slow
            ], capture_output=True)
            overlaid_videos[-1] = last_scene_slow
        
        # 4. å‹•ç”»ã‚’çµåˆï¼ˆfilter_complexæ–¹å¼ï¼‰
        inputs = []
        for v in overlaid_videos:
            inputs.extend(["-i", v])
        
        n = len(overlaid_videos)
        filter_str = "".join([f"[{i}:v]" for i in range(n)]) + f"concat=n={n}:v=1:a=0[v]"
        
        concat_video_path = str(temp_dir / "concat.mp4")
        cmd = ["ffmpeg", "-y"] + inputs + [
            "-filter_complex", filter_str,
            "-map", "[v]",
            "-c:v", "libx264", "-preset", "fast", "-crf", "18",
            concat_video_path
        ]
        subprocess.run(cmd, capture_output=True)
        console.print("  âœ… å‹•ç”»çµåˆå®Œäº†")
        
        # 5. éŸ³å£°ã‚’è¿½åŠ 
        final_path = str(self.dirs["final"] / f"{output_prefix}_final.mp4")
        subprocess.run([
            "ffmpeg", "-y",
            "-i", concat_video_path,
            "-i", audio_path,
            "-c:v", "copy",
            "-c:a", "aac", "-b:a", "192k",
            "-shortest",
            final_path
        ], capture_output=True)
        
        console.print(f"\n[green]ğŸ‰ å®Œæˆ: {final_path}[/green]")
        
        return final_path
    
    def run(
        self,
        headline: str,
        sub_headline: str = "",
        scenes_data: list[dict] = None,
        closing_text: str = "",
        article_text: str = "",  # å¾Œæ–¹äº’æ›ç”¨
        output_prefix: Optional[str] = None,
        is_breaking: bool = True,
    ) -> NewsVideoResult:
        """ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å…¨ä½“ã‚’å®Ÿè¡Œ
        
        Args:
            headline: ãƒ˜ãƒƒãƒ‰ãƒ©ã‚¤ãƒ³
            sub_headline: ã‚µãƒ–ãƒ˜ãƒƒãƒ‰ãƒ©ã‚¤ãƒ³
            scenes_data: ã‚·ãƒ¼ãƒ³æ§‹æˆãƒ‡ãƒ¼ã‚¿ï¼ˆæ–°å½¢å¼ï¼‰
            closing_text: ç· ã‚ãƒŠãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ï¼ˆçœç•¥å¯ï¼‰
            article_text: è¨˜äº‹æœ¬æ–‡ï¼ˆå¾Œæ–¹äº’æ›ç”¨ã€scenes_dataãŒãªã„å ´åˆã«ä½¿ç”¨ï¼‰
            output_prefix: å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«åãƒ—ãƒ¬ãƒ•ã‚£ãƒƒã‚¯ã‚¹
            is_breaking: BREAKING NEWSãƒãƒŠãƒ¼è¡¨ç¤º
        """
        
        if output_prefix is None:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            output_prefix = f"news_{timestamp}"
        
        console.print("\n" + "=" * 50)
        console.print(f"[bold]ğŸ“° ãƒ‹ãƒ¥ãƒ¼ã‚¹å‹•ç”»ç”Ÿæˆ: {headline[:30]}...[/bold]")
        console.print("=" * 50)
        
        try:
            # ã‚·ãƒ¼ãƒ³æ§‹æˆãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚‹å ´åˆã¯æ–°ãƒ•ãƒ­ãƒ¼
            if scenes_data and len(scenes_data) > 0:
                return self._run_with_scene_sync(
                    headline=headline,
                    sub_headline=sub_headline,
                    scenes_data=scenes_data,
                    closing_text=closing_text,
                    output_prefix=output_prefix,
                    is_breaking=is_breaking,
                )
            
            # å¾Œæ–¹äº’æ›: å¾“æ¥ã®ãƒ•ãƒ­ãƒ¼ï¼ˆarticle_textã‹ã‚‰åˆ†æï¼‰
            if not article_text:
                return NewsVideoResult(
                    success=False,
                    error_message="scenes_data ã¾ãŸã¯ article_text ãŒå¿…è¦ã§ã™",
                )
            
            # 1. è¨˜äº‹åˆ†æ
            scenes = self.analyze_article(article_text, headline)
            
            # 2. ç”»åƒç”Ÿæˆ
            scenes = self.generate_scene_images(scenes, output_prefix)
            
            # 3. å‹•ç”»ç”Ÿæˆ
            scenes = self.generate_scene_videos(scenes, output_prefix)
            
            # 4. ãƒŠãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ç”Ÿæˆï¼ˆè¨˜äº‹å…¨æ–‡ã‚’ä½¿ç”¨ï¼‰
            audio_path, audio_duration = self.generate_narration(
                article_text, output_prefix, closing_text=closing_text
            )
            
            # 5. æœ€çµ‚åˆæˆï¼ˆéŸ³å£°é•·ã«åˆã‚ã›ã¦ã‚¹ãƒ­ãƒ¼èª¿æ•´ï¼‰
            final_path = self.compose_final_video(
                scenes=scenes,
                audio_path=audio_path,
                audio_duration=audio_duration,
                headline=headline,
                sub_headline=sub_headline,
                output_prefix=output_prefix,
                is_breaking=is_breaking,
            )
            
            # å‹•ç”»ã®é•·ã•ã‚’å–å¾—
            probe = subprocess.run(
                ["ffprobe", "-v", "error", "-show_entries", "format=duration",
                 "-of", "default=noprint_wrappers=1:nokey=1", final_path],
                capture_output=True, text=True
            )
            duration = float(probe.stdout.strip())
            
            return NewsVideoResult(
                success=True,
                video_path=final_path,
                scenes=scenes,
                audio_path=audio_path,
                duration_seconds=duration,
            )
            
        except Exception as e:
            console.print(f"[red]âŒ ã‚¨ãƒ©ãƒ¼: {e}[/red]")
            import traceback
            traceback.print_exc()
            return NewsVideoResult(
                success=False,
                error_message=str(e),
            )
    
    def _run_with_scene_sync(
        self,
        headline: str,
        sub_headline: str,
        scenes_data: list[dict],
        closing_text: str,
        output_prefix: str,
        is_breaking: bool,
    ) -> NewsVideoResult:
        """ã‚·ãƒ¼ãƒ³åŒæœŸãƒ•ãƒ­ãƒ¼: å„ã‚·ãƒ¼ãƒ³ã®ãƒŠãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã¨æ˜ åƒã‚’åŒæœŸã•ã›ã‚‹"""
        
        console.print(f"\n[cyan]ğŸ¬ ã‚·ãƒ¼ãƒ³åŒæœŸãƒ¢ãƒ¼ãƒ‰ ({len(scenes_data)}ã‚·ãƒ¼ãƒ³)[/cyan]")
        
        # 1. scenes_dataã‹ã‚‰Sceneã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’ä½œæˆ
        scenes = []
        for i, sd in enumerate(scenes_data):
            # visual_descriptionã‹ã‚‰ç”»åƒãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ç”Ÿæˆ
            visual_desc = sd.get("visual_description", sd.get("title", ""))
            image_prompt = self._create_image_prompt(visual_desc, headline)
            
            scene = Scene(
                index=i,
                description=visual_desc,
                image_prompt=image_prompt,
                video_prompt=f"Slow cinematic camera movement, {visual_desc}",
                subtitle=sd.get("narration", "")[:30],  # å­—å¹•ã¯çŸ­ã
            )
            # ãƒŠãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’ä¿æŒ
            scene.narration_text = sd.get("narration", "")
            scenes.append(scene)
            console.print(f"  ã‚·ãƒ¼ãƒ³{i+1}: {visual_desc[:40]}...")
        
        # 2. ç”»åƒç”Ÿæˆ
        scenes = self.generate_scene_images(scenes, output_prefix)
        
        # 3. å‹•ç”»ç”Ÿæˆ
        scenes = self.generate_scene_videos(scenes, output_prefix)
        
        # 4. ã‚·ãƒ¼ãƒ³ã”ã¨ã«ãƒŠãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ç”Ÿæˆ
        console.print("\n[cyan]ğŸ¤ ã‚·ãƒ¼ãƒ³åˆ¥ãƒŠãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ç”Ÿæˆä¸­...[/cyan]")
        scene_audios = []
        total_audio_duration = 0
        
        for scene in scenes:
            narration_text = getattr(scene, 'narration_text', scene.subtitle)
            if not narration_text:
                continue
                
            audio_path = str(self.dirs["audio"] / f"{output_prefix}_scene{scene.index + 1}.mp3")
            result = self.narration_gen.generate(text=narration_text, output_path=audio_path)
            
            if result.success:
                scene.audio_path = audio_path
                scene.audio_duration = result.duration_seconds
                total_audio_duration += result.duration_seconds
                scene_audios.append(audio_path)
                console.print(f"  âœ… ã‚·ãƒ¼ãƒ³{scene.index + 1}: {result.duration_seconds:.1f}ç§’")
            else:
                console.print(f"  âŒ ã‚·ãƒ¼ãƒ³{scene.index + 1}: éŸ³å£°ç”Ÿæˆå¤±æ•—")
        
        # 5. ç· ã‚ãƒŠãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³
        if closing_text:
            closing_path = str(self.dirs["audio"] / f"{output_prefix}_closing.mp3")
            closing_result = self.narration_gen.generate(text=closing_text, output_path=closing_path)
            if closing_result.success:
                scene_audios.append(closing_path)
                total_audio_duration += closing_result.duration_seconds
                console.print(f"  âœ… ç· ã‚: {closing_result.duration_seconds:.1f}ç§’")
        
        # 6. å…¨éŸ³å£°ã‚’çµåˆ
        console.print("\n[cyan]ğŸ”Š éŸ³å£°çµåˆä¸­...[/cyan]")
        combined_audio = str(self.dirs["audio"] / f"{output_prefix}_combined.mp3")
        
        if len(scene_audios) > 1:
            # ffmpegã§çµåˆ
            concat_list = str(self.dirs["temp"] / "audio_concat.txt")
            with open(concat_list, "w") as f:
                for ap in scene_audios:
                    f.write(f"file '{ap}'\n")
            
            subprocess.run([
                "ffmpeg", "-y", "-f", "concat", "-safe", "0",
                "-i", concat_list, "-c", "copy", combined_audio
            ], capture_output=True)
        else:
            combined_audio = scene_audios[0] if scene_audios else None
        
        console.print(f"  âœ… åˆè¨ˆéŸ³å£°: {total_audio_duration:.1f}ç§’")
        
        # 6.5. BGMãƒŸãƒƒã‚¯ã‚¹
        final_audio = combined_audio
        if combined_audio:
            # ãƒŠãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰ãƒ ãƒ¼ãƒ‰ã‚’æ¤œå‡º
            all_narration = " ".join([getattr(s, 'narration_text', '') for s in scenes])
            mood = self.bgm_manager.detect_mood(headline, all_narration)
            bgm_track = self.bgm_manager.get_bgm(mood)
            
            if bgm_track and bgm_track.exists():
                console.print(f"\n[cyan]ğŸµ BGMãƒŸãƒƒã‚¯ã‚¹ä¸­... ({mood.value})[/cyan]")
                mixed_audio = str(self.dirs["audio"] / f"{output_prefix}_mixed.mp3")
                
                if self.bgm_manager.mix_audio(
                    narration_path=combined_audio,
                    bgm_path=bgm_track.path,
                    output_path=mixed_audio,
                    narration_volume=1.0,
                    bgm_volume=0.12,  # BGMã¯æ§ãˆã‚
                ):
                    final_audio = mixed_audio
                    console.print(f"  âœ… BGMè¿½åŠ : {bgm_track.name}")
                else:
                    console.print(f"  âš ï¸ BGMãƒŸãƒƒã‚¯ã‚¹å¤±æ•—ã€ãƒŠãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã®ã¿ä½¿ç”¨")
            else:
                console.print(f"  â„¹ï¸ BGMãªã—ï¼ˆ{mood.value}ç”¨BGMæœªè¨­å®šï¼‰")
        
        # 7. æœ€çµ‚åˆæˆï¼ˆã‚·ãƒ¼ãƒ³ã”ã¨ã«éŸ³å£°é•·ã«åˆã‚ã›ã‚‹ï¼‰
        final_path = self._compose_scene_synced_video(
            scenes=scenes,
            combined_audio=final_audio,  # BGMãƒŸãƒƒã‚¯ã‚¹æ¸ˆã¿éŸ³å£°
            total_audio_duration=total_audio_duration,
            headline=headline,
            sub_headline=sub_headline,
            output_prefix=output_prefix,
            is_breaking=is_breaking,
        )
        
        # å‹•ç”»ã®é•·ã•ã‚’å–å¾—
        probe = subprocess.run(
            ["ffprobe", "-v", "error", "-show_entries", "format=duration",
             "-of", "default=noprint_wrappers=1:nokey=1", final_path],
            capture_output=True, text=True
        )
        duration = float(probe.stdout.strip()) if probe.stdout.strip() else total_audio_duration
        
        return NewsVideoResult(
            success=True,
            video_path=final_path,
            scenes=scenes,
            audio_path=combined_audio,
            duration_seconds=duration,
        )
    
    def _create_image_prompt(self, visual_desc: str, headline: str) -> str:
        """visual_descriptionã‹ã‚‰ç”»åƒãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ç”Ÿæˆ"""
        return f"Photorealistic, cinematic lighting, 4K quality, {visual_desc}, related to: {headline}"
    
    def _compose_scene_synced_video(
        self,
        scenes: list[Scene],
        combined_audio: str,
        total_audio_duration: float,
        headline: str,
        sub_headline: str,
        output_prefix: str,
        is_breaking: bool,
    ) -> str:
        """ã‚·ãƒ¼ãƒ³åŒæœŸã§æœ€çµ‚å‹•ç”»ã‚’åˆæˆ"""
        
        console.print("\n[cyan]ğŸ¬ ã‚·ãƒ¼ãƒ³åŒæœŸåˆæˆä¸­...[/cyan]")
        
        valid_scenes = [s for s in scenes if s.video_path]
        if not valid_scenes:
            raise ValueError("æœ‰åŠ¹ãªã‚·ãƒ¼ãƒ³å‹•ç”»ãŒã‚ã‚Šã¾ã›ã‚“")
        
        # å„ã‚·ãƒ¼ãƒ³ã®ç›®æ¨™æ™‚é–“ã‚’è¨ˆç®—
        num_scenes = len(valid_scenes)
        base_duration_per_scene = total_audio_duration / num_scenes
        
        console.print(f"  ã‚·ãƒ¼ãƒ³æ•°: {num_scenes}, å„ã‚·ãƒ¼ãƒ³ç›®æ¨™: {base_duration_per_scene:.1f}ç§’")
        
        # å‹•ç”»ã‚µã‚¤ã‚ºã‚’å–å¾—
        probe = subprocess.run(
            ["ffprobe", "-v", "error", "-select_streams", "v:0",
             "-show_entries", "stream=width,height", "-of", "csv=p=0",
             valid_scenes[0].video_path],
            capture_output=True, text=True
        )
        width, height = map(int, probe.stdout.strip().split(','))
        
        temp_dir = self.dirs["temp"]
        
        # å„ã‚·ãƒ¼ãƒ³ã‚’ç›®æ¨™æ™‚é–“ã«èª¿æ•´ã—ã¦ã‚ªãƒ¼ãƒãƒ¼ãƒ¬ã‚¤è¿½åŠ 
        adjusted_videos = []
        
        for i, scene in enumerate(valid_scenes):
            # ã‚·ãƒ¼ãƒ³åˆ¥ã®éŸ³å£°ãŒã‚ã‚Œã°ã€ãã®é•·ã•ã«åˆã‚ã›ã‚‹
            target_duration = getattr(scene, 'audio_duration', base_duration_per_scene)
            
            # å‹•ç”»ã®å®Ÿéš›ã®é•·ã•ã‚’å–å¾—
            probe = subprocess.run(
                ["ffprobe", "-v", "error", "-show_entries", "format=duration",
                 "-of", "default=noprint_wrappers=1:nokey=1", scene.video_path],
                capture_output=True, text=True
            )
            actual_duration = float(probe.stdout.strip())
            
            # ã‚¹ãƒ­ãƒ¼ç‡ã‚’è¨ˆç®—ï¼ˆæœ€å¤§2å€ã¾ã§ï¼‰
            slowdown = min(target_duration / actual_duration, 2.0)
            
            # ã‚ªãƒ¼ãƒãƒ¼ãƒ¬ã‚¤ä½œæˆ
            overlay_path = str(temp_dir / f"overlay_{i}.png")
            self.compositor.create_transparent_overlay(
                output_path=overlay_path,
                headline=headline if i == 0 else "",
                sub_headline=sub_headline if i == 0 else "",
                subtitle=scene.subtitle,
                is_breaking=is_breaking and i == 0,
                width=width,
                height=height,
            )
            
            # å‹•ç”»èª¿æ•´ï¼ˆã‚¹ãƒ­ãƒ¼ + ã‚ªãƒ¼ãƒãƒ¼ãƒ¬ã‚¤ï¼‰
            adjusted_path = str(temp_dir / f"adjusted_{i}.mp4")
            
            filter_complex = f"[0:v]setpts={slowdown}*PTS[slowed];[slowed][1:v]overlay=0:0"
            
            subprocess.run([
                "ffmpeg", "-y",
                "-i", scene.video_path,
                "-i", overlay_path,
                "-filter_complex", filter_complex,
                "-t", str(target_duration),
                "-c:v", "libx264", "-preset", "fast",
                "-an",
                adjusted_path
            ], capture_output=True)
            
            adjusted_videos.append(adjusted_path)
            console.print(f"  âœ… ã‚·ãƒ¼ãƒ³{i+1}: {actual_duration:.1f}ç§’ â†’ {target_duration:.1f}ç§’ (x{slowdown:.2f})")
        
        # å‹•ç”»ã‚’çµåˆ
        concat_list = str(temp_dir / "video_concat.txt")
        with open(concat_list, "w") as f:
            for vp in adjusted_videos:
                f.write(f"file '{vp}'\n")
        
        concat_video = str(temp_dir / f"{output_prefix}_concat.mp4")
        subprocess.run([
            "ffmpeg", "-y", "-f", "concat", "-safe", "0",
            "-i", concat_list, "-c", "copy", concat_video
        ], capture_output=True)
        
        # éŸ³å£°ã‚’è¿½åŠ 
        final_path = str(self.dirs["final"] / f"{output_prefix}_final.mp4")
        
        if combined_audio:
            subprocess.run([
                "ffmpeg", "-y",
                "-i", concat_video,
                "-i", combined_audio,
                "-c:v", "copy",
                "-c:a", "aac", "-b:a", "192k",
                "-shortest",
                final_path
            ], capture_output=True)
        else:
            subprocess.run(["cp", concat_video, final_path])
        
        console.print(f"\n[green]ğŸ‰ å®Œæˆ: {final_path}[/green]")
        
        return final_path


# CLIç”¨
if __name__ == "__main__":
    import sys
    
    pipeline = NewsVideoPipeline()
    
    # ãƒ†ã‚¹ãƒˆç”¨
    result = pipeline.run(
        article_text="""
        ã‚¹ãƒšã‚¤ãƒ³ã§è¡Œæ–¹ä¸æ˜ã«ãªã£ãŸçŒ«ãŒã€5ãƒ¶æœˆã‹ã‘ã¦250ã‚­ãƒ­ã‚’æ­©ãã€
        ãƒ•ãƒ©ãƒ³ã‚¹ã®è‡ªå®…ã«å¸°é‚„ã—ã¾ã—ãŸã€‚é£¼ã„ä¸»ã®ãƒ•ã‚¡ãƒ“ã‚¢ãƒ³ã•ã‚“ã¯ã€
        æ„›çŒ«ãƒŸãƒŒã‚·ãƒ¥ãŒæˆ»ã£ã¦ããŸæ™‚ã€ä¿¡ã˜ã‚‰ã‚Œãªã‹ã£ãŸã¨èªã£ã¦ã„ã¾ã™ã€‚
        çŒ«ã¯å°‘ã—ç—©ã›ã¦ã„ã¾ã—ãŸãŒã€å…ƒæ°—ãªæ§˜å­ã§ã—ãŸã€‚
        """,
        headline="çŒ«ãŒ250kmæ­©ã„ã¦ã‚¹ãƒšã‚¤ãƒ³ã‹ã‚‰ãƒ•ãƒ©ãƒ³ã‚¹ã®è‡ªå®…ã«å¸°é‚„",
        sub_headline="5ãƒ¶æœˆã‹ã‘ã¦155ãƒã‚¤ãƒ«ã‚’è¸ç ´",
        output_prefix="cat_journey",
    )
    
    print(f"\nçµæœ: {'æˆåŠŸ' if result.success else 'å¤±æ•—'}")
    if result.success:
        print(f"å‹•ç”»: {result.video_path}")
        print(f"é•·ã•: {result.duration_seconds:.1f}ç§’")
