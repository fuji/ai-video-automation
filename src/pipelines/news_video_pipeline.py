"""
ãƒ‹ãƒ¥ãƒ¼ã‚¹å‹•ç”»ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³

è¨˜äº‹ã‹ã‚‰è¤‡æ•°ã‚·ãƒ¼ãƒ³ã®ãƒ‹ãƒ¥ãƒ¼ã‚¹å‹•ç”»ã‚’è‡ªå‹•ç”Ÿæˆ
"""

import os
import subprocess
import json
from pathlib import Path
from dataclasses import dataclass, field
from typing import Optional
from datetime import datetime

from PIL import Image, ImageDraw, ImageFont
from rich.console import Console
import google.generativeai as genai

import fal_client
import httpx
import time

from src.generators.image_generator import FluxImageGenerator
from src.config import config
from src.generators.edge_tts_generator import EdgeTTSGenerator  # ç„¡æ–™TTS
from src.editors.news_graphics import NewsGraphicsCompositor
from src.config import IMAGES_DIR, VIDEOS_DIR, OUTPUT_DIR

# AUDIO_DIR ãŒãªã‘ã‚Œã°ä½œæˆ
AUDIO_DIR = OUTPUT_DIR / "audio"
AUDIO_DIR.mkdir(exist_ok=True)

console = Console()


@dataclass
class Scene:
    """ã‚·ãƒ¼ãƒ³æƒ…å ±"""
    index: int
    description: str  # ã‚·ãƒ¼ãƒ³ã®èª¬æ˜
    image_prompt: str  # Fluxç”¨ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
    video_prompt: str  # Lumaç”¨ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
    subtitle: str  # ã“ã®ã‚·ãƒ¼ãƒ³ã®å­—å¹•
    image_path: Optional[str] = None
    video_path: Optional[str] = None


@dataclass
class NewsVideoResult:
    """ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³çµæœ"""
    success: bool
    video_path: Optional[str] = None
    scenes: list[Scene] = field(default_factory=list)
    audio_path: Optional[str] = None
    duration_seconds: float = 0.0
    error_message: Optional[str] = None


class NewsVideoPipeline:
    """ãƒ‹ãƒ¥ãƒ¼ã‚¹å‹•ç”»ç”Ÿæˆãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³"""
    
    def __init__(
        self,
        channel_name: str = "FJ News 24",
        num_scenes: int = 3,
        scene_duration: float = 5.0,
    ):
        self.channel_name = channel_name
        self.num_scenes = num_scenes
        self.scene_duration = scene_duration
        
        # ã‚¸ã‚§ãƒãƒ¬ãƒ¼ã‚¿ãƒ¼åˆæœŸåŒ–
        self.image_gen = FluxImageGenerator()
        self.narration_gen = EdgeTTSGenerator()  # ç„¡æ–™TTS (Edge TTS)
        self.compositor = NewsGraphicsCompositor(channel_name=channel_name)
        
        # Gemini for scene analysis
        genai.configure(api_key=config.gemini.api_key)
        self.gemini = genai.GenerativeModel("gemini-2.0-flash")
        
        # FAL API key for Luma
        os.environ["FAL_KEY"] = config.fal.api_key
        
        console.print(f"[green]NewsVideoPipeline initialized[/green]")
        console.print(f"  Channel: {channel_name}")
        console.print(f"  Scenes: {num_scenes} x {scene_duration}s = {num_scenes * scene_duration}s")
    
    def analyze_article(
        self,
        article_text: str,
        headline: str,
    ) -> list[Scene]:
        """è¨˜äº‹ã‚’åˆ†æã—ã¦è¤‡æ•°ã‚·ãƒ¼ãƒ³ã«åˆ†è§£"""
        
        prompt = f"""ä»¥ä¸‹ã®ãƒ‹ãƒ¥ãƒ¼ã‚¹è¨˜äº‹ã‚’{self.num_scenes}ã¤ã®ã‚·ãƒ¼ãƒ³ã«åˆ†è§£ã—ã¦ãã ã•ã„ã€‚

# è¨˜äº‹
ã‚¿ã‚¤ãƒˆãƒ«: {headline}
æœ¬æ–‡: {article_text}

# å‡ºåŠ›å½¢å¼ (JSON)
å„ã‚·ãƒ¼ãƒ³ã«ã¤ã„ã¦ä»¥ä¸‹ã‚’ç”Ÿæˆ:
- description: ã‚·ãƒ¼ãƒ³ã®èª¬æ˜ï¼ˆæ—¥æœ¬èªã€1æ–‡ï¼‰
- image_prompt: Fluxç”»åƒç”Ÿæˆç”¨ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆï¼ˆè‹±èªã€å…·ä½“çš„ã§è¦–è¦šçš„ãªæå†™ã€50èªä»¥å†…ï¼‰
- video_prompt: Lumaå‹•ç”»ç”Ÿæˆç”¨ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆï¼ˆè‹±èªã€ã‚«ãƒ¡ãƒ©ãƒ¯ãƒ¼ã‚¯ã‚„å‹•ãã®æŒ‡ç¤ºã€20èªä»¥å†…ï¼‰
- subtitle: ã“ã®ã‚·ãƒ¼ãƒ³ã®ãƒŠãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³å­—å¹•ï¼ˆæ—¥æœ¬èªã€15-25æ–‡å­—ï¼‰

# æ³¨æ„
- image_promptã¯ãƒ•ã‚©ãƒˆãƒªã‚¢ãƒªã‚¹ãƒ†ã‚£ãƒƒã‚¯ãªã‚¹ã‚¿ã‚¤ãƒ«ã§
- å„ã‚·ãƒ¼ãƒ³ãŒç‰©èªã¨ã—ã¦ç¹‹ãŒã‚‹ã‚ˆã†ã«
- æœ€åˆã®ã‚·ãƒ¼ãƒ³ã¯çŠ¶æ³èª¬æ˜ã€ä¸­é–“ã¯å±•é–‹ã€æœ€å¾Œã¯çµæœ«

```json
{{
  "scenes": [
    {{
      "description": "...",
      "image_prompt": "...",
      "video_prompt": "...",
      "subtitle": "..."
    }}
  ]
}}
```"""

        console.print("\n[cyan]ğŸ“ è¨˜äº‹ã‚’åˆ†æä¸­...[/cyan]")
        
        response = self.gemini.generate_content(prompt)
        
        # JSONã‚’æŠ½å‡º
        content = response.text
        json_start = content.find("{")
        json_end = content.rfind("}") + 1
        json_str = content[json_start:json_end]
        
        data = json.loads(json_str)
        
        scenes = []
        for i, scene_data in enumerate(data["scenes"]):
            scene = Scene(
                index=i,
                description=scene_data["description"],
                image_prompt=scene_data["image_prompt"],
                video_prompt=scene_data["video_prompt"],
                subtitle=scene_data["subtitle"],
            )
            scenes.append(scene)
            console.print(f"  ã‚·ãƒ¼ãƒ³{i+1}: {scene.description}")
        
        return scenes
    
    def generate_scene_images(
        self,
        scenes: list[Scene],
        output_prefix: str,
    ) -> list[Scene]:
        """å„ã‚·ãƒ¼ãƒ³ã®ç”»åƒã‚’ç”Ÿæˆ"""
        
        console.print("\n[cyan]ğŸ–¼ï¸ ã‚·ãƒ¼ãƒ³ç”»åƒã‚’ç”Ÿæˆä¸­...[/cyan]")
        
        for scene in scenes:
            output_name = f"{output_prefix}_scene{scene.index + 1}"
            
            result = self.image_gen.generate(
                prompt=scene.image_prompt,
                output_name=output_name,
                image_size="portrait_16_9",  # ç¸¦å‹•ç”»ç”¨
            )
            
            if result.success:
                scene.image_path = result.file_path
                console.print(f"  âœ… ã‚·ãƒ¼ãƒ³{scene.index + 1}: {result.file_path}")
            else:
                console.print(f"  âŒ ã‚·ãƒ¼ãƒ³{scene.index + 1}: {result.error_message}")
        
        return scenes
    
    def generate_scene_videos(
        self,
        scenes: list[Scene],
        output_prefix: str,
    ) -> list[Scene]:
        """å„ã‚·ãƒ¼ãƒ³ã®å‹•ç”»ã‚’ç”Ÿæˆï¼ˆLuma Dream Machine via fal.aiï¼‰"""
        
        console.print("\n[cyan]ğŸ¬ ã‚·ãƒ¼ãƒ³å‹•ç”»ã‚’ç”Ÿæˆä¸­ (Luma)...[/cyan]")
        
        for scene in scenes:
            if not scene.image_path:
                console.print(f"  âš ï¸ ã‚·ãƒ¼ãƒ³{scene.index + 1}: ç”»åƒãŒã‚ã‚Šã¾ã›ã‚“")
                continue
            
            output_path = str(VIDEOS_DIR / f"{output_prefix}_scene{scene.index + 1}.mp4")
            
            try:
                # ç”»åƒã‚’fal.aiã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
                image_url = fal_client.upload_file(scene.image_path)
                console.print(f"  ğŸ“¤ ã‚·ãƒ¼ãƒ³{scene.index + 1}: ç”»åƒã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰å®Œäº†")
                
                # Luma APIå‘¼ã³å‡ºã—
                result = fal_client.subscribe(
                    "fal-ai/luma-dream-machine/image-to-video",
                    arguments={
                        "prompt": scene.video_prompt,
                        "image_url": image_url,
                        "aspect_ratio": "9:16",
                    },
                    with_logs=False,
                )
                
                # å‹•ç”»ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
                video_url = result["video"]["url"]
                response = httpx.get(video_url)
                with open(output_path, "wb") as f:
                    f.write(response.content)
                
                scene.video_path = output_path
                console.print(f"  âœ… ã‚·ãƒ¼ãƒ³{scene.index + 1}: {output_path}")
                
            except Exception as e:
                console.print(f"  âŒ ã‚·ãƒ¼ãƒ³{scene.index + 1}: {str(e)}")
        
        return scenes
    
    def generate_narration(
        self,
        scenes: list[Scene],
        output_prefix: str,
        closing_text: str = "",
    ) -> tuple[str, float]:
        """ã‚·ãƒ¼ãƒ³ã®å­—å¹•ã‹ã‚‰ãƒŠãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³éŸ³å£°ã‚’ç”Ÿæˆï¼ˆç· ã‚ãƒŠãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³å«ã‚€ï¼‰
        
        Returns:
            tuple: (audio_path, total_duration)
        """
        
        console.print("\n[cyan]ğŸ¤ ãƒŠãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ç”Ÿæˆä¸­...[/cyan]")
        
        # å…¨ã‚·ãƒ¼ãƒ³ã®å­—å¹•ã‚’çµåˆ
        full_text = "ã€‚".join([scene.subtitle for scene in scenes]) + "ã€‚"
        
        main_path = str(AUDIO_DIR / f"{output_prefix}_narration.mp3")
        result = self.narration_gen.generate(text=full_text, output_path=main_path)
        
        if not result.success:
            console.print(f"  âŒ éŸ³å£°ç”Ÿæˆå¤±æ•—: {result.error_message}")
            return None, 0
        
        console.print(f"  âœ… æœ¬ç·¨éŸ³å£°: {result.file_path} ({result.duration_seconds:.1f}ç§’)")
        
        # ç· ã‚ãƒŠãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ãŒã‚ã‚Œã°è¿½åŠ 
        if closing_text:
            console.print("  ğŸ¤ ç· ã‚ãƒŠãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ç”Ÿæˆä¸­...")
            closing_path = str(AUDIO_DIR / f"{output_prefix}_closing.mp3")
            closing_result = self.narration_gen.generate(text=closing_text, output_path=closing_path)
            
            if closing_result.success:
                console.print(f"  âœ… ç· ã‚éŸ³å£°: {closing_result.file_path} ({closing_result.duration_seconds:.1f}ç§’)")
                
                # éŸ³å£°ã‚’çµåˆ
                combined_path = str(AUDIO_DIR / f"{output_prefix}_full.mp3")
                subprocess.run([
                    "ffmpeg", "-y",
                    "-i", main_path, "-i", closing_path,
                    "-filter_complex", "[0:a][1:a]concat=n=2:v=0:a=1[a]",
                    "-map", "[a]", combined_path
                ], capture_output=True)
                
                # çµåˆå¾Œã®é•·ã•ã‚’å–å¾—
                probe = subprocess.run(
                    ["ffprobe", "-v", "error", "-show_entries", "format=duration",
                     "-of", "default=noprint_wrappers=1:nokey=1", combined_path],
                    capture_output=True, text=True
                )
                total_duration = float(probe.stdout.strip())
                console.print(f"  âœ… åˆè¨ˆéŸ³å£°: {total_duration:.1f}ç§’")
                return combined_path, total_duration
        
        return result.file_path, result.duration_seconds
    
    def compose_final_video(
        self,
        scenes: list[Scene],
        audio_path: str,
        audio_duration: float,
        headline: str,
        sub_headline: str,
        output_prefix: str,
        is_breaking: bool = True,
    ) -> str:
        """å…¨ã‚·ãƒ¼ãƒ³ã‚’çµåˆã—ã¦æœ€çµ‚å‹•ç”»ã‚’ä½œæˆï¼ˆéŸ³å£°é•·ã«åˆã‚ã›ã¦ã‚¹ãƒ­ãƒ¼èª¿æ•´ï¼‰"""
        
        console.print("\n[cyan]ğŸ¬ æœ€çµ‚å‹•ç”»ã‚’åˆæˆä¸­...[/cyan]")
        
        # å‹•ç”»ãŒã‚ã‚‹ã‚·ãƒ¼ãƒ³ã ã‘æŠ½å‡º
        valid_scenes = [s for s in scenes if s.video_path]
        if not valid_scenes:
            raise ValueError("æœ‰åŠ¹ãªå‹•ç”»ãŒã‚ã‚Šã¾ã›ã‚“")
        
        # æœ€åˆã®å‹•ç”»ã‹ã‚‰ã‚µã‚¤ã‚ºã‚’å–å¾—
        probe = subprocess.run(
            ["ffprobe", "-v", "error", "-select_streams", "v:0",
             "-show_entries", "stream=width,height", "-of", "csv=p=0",
             valid_scenes[0].video_path],
            capture_output=True, text=True
        )
        width, height = map(int, probe.stdout.strip().split(','))
        
        # ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ç”¨ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
        temp_dir = VIDEOS_DIR / "temp"
        temp_dir.mkdir(exist_ok=True)
        
        # 1. å„ã‚·ãƒ¼ãƒ³ã«ã‚ªãƒ¼ãƒãƒ¼ãƒ¬ã‚¤ã¨å­—å¹•ã‚’è¿½åŠ 
        overlaid_videos = []
        
        for scene in valid_scenes:
            # ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚ªãƒ¼ãƒãƒ¼ãƒ¬ã‚¤ä½œæˆ
            overlay_path = str(temp_dir / f"overlay_{scene.index}.png")
            self.compositor.create_transparent_overlay(
                width=width, height=height,
                headline=headline,
                sub_headline=sub_headline,
                is_breaking=is_breaking,
                style="solid",
                output_path=overlay_path,
            )
            
            # å­—å¹•ã‚’è¿½åŠ 
            overlay_img = Image.open(overlay_path).convert("RGBA")
            draw = ImageDraw.Draw(overlay_img)
            
            font = ImageFont.truetype(
                "/System/Library/Fonts/ãƒ’ãƒ©ã‚®ãƒè§’ã‚´ã‚·ãƒƒã‚¯ W3.ttc",
                int(height * 0.032)
            )
            
            # å­—å¹•ã‚’è¤‡æ•°è¡Œã«åˆ†å‰²ï¼ˆé•·ã„å ´åˆï¼‰
            subtitle = scene.subtitle
            if len(subtitle) > 15:
                mid = len(subtitle) // 2
                for i in range(mid, 0, -1):
                    if subtitle[i] in 'ãŒã®ã‚’ã«ã¯ã§ã¨ã€ã€‚':
                        mid = i + 1
                        break
                lines = [subtitle[:mid], subtitle[mid:]]
            else:
                lines = [subtitle]
            
            margin_x = int(width * 0.10)
            max_text_width = width - margin_x * 2
            line_height = int(height * 0.045)
            total_text_height = len(lines) * line_height
            start_y = (height - total_text_height) // 2
            
            for i, line in enumerate(lines):
                bbox = draw.textbbox((0, 0), line, font=font)
                text_w = bbox[2] - bbox[0]
                text_x = margin_x + (max_text_width - text_w) // 2
                y = start_y + i * line_height
                draw.text(
                    (text_x, y), line, font=font,
                    fill=(255, 255, 255, 255),
                    stroke_width=3,
                    stroke_fill=(0, 0, 0, 255)
                )
            
            scene_overlay_path = str(temp_dir / f"scene_overlay_{scene.index}.png")
            overlay_img.save(scene_overlay_path, "PNG")
            
            # FFmpegã§ã‚ªãƒ¼ãƒãƒ¼ãƒ¬ã‚¤åˆæˆ
            overlaid_path = str(temp_dir / f"overlaid_{scene.index}.mp4")
            subprocess.run([
                "ffmpeg", "-y",
                "-i", scene.video_path,
                "-i", scene_overlay_path,
                "-filter_complex", "[0:v][1:v]overlay=0:0",
                "-c:v", "libx264", "-preset", "fast", "-crf", "18",
                "-an", overlaid_path
            ], capture_output=True)
            
            overlaid_videos.append(overlaid_path)
            console.print(f"  âœ… ã‚·ãƒ¼ãƒ³{scene.index + 1} ã‚ªãƒ¼ãƒãƒ¼ãƒ¬ã‚¤é©ç”¨")
        
        # 2. å„ã‚·ãƒ¼ãƒ³ã®é•·ã•ã‚’å–å¾—
        def get_duration(path):
            probe = subprocess.run(
                ["ffprobe", "-v", "error", "-show_entries", "format=duration",
                 "-of", "default=noprint_wrappers=1:nokey=1", path],
                capture_output=True, text=True
            )
            return float(probe.stdout.strip())
        
        video_durations = [get_duration(v) for v in overlaid_videos]
        total_video_duration = sum(video_durations)
        
        console.print(f"  å‹•ç”»åˆè¨ˆ: {total_video_duration:.1f}ç§’, éŸ³å£°: {audio_duration:.1f}ç§’")
        
        # 3. éŸ³å£°ãŒé•·ã„å ´åˆã€æœ€å¾Œã®ã‚·ãƒ¼ãƒ³ã‚’ã‚¹ãƒ­ãƒ¼ã«ã—ã¦èª¿æ•´
        if audio_duration > total_video_duration:
            other_scenes_duration = sum(video_durations[:-1])
            needed_last_scene = audio_duration - other_scenes_duration + 0.3
            slowdown_factor = needed_last_scene / video_durations[-1]
            
            console.print(f"  æœ€å¾Œã®ã‚·ãƒ¼ãƒ³ã‚’ {slowdown_factor:.2f}x ã‚¹ãƒ­ãƒ¼ã«èª¿æ•´")
            
            # æœ€å¾Œã®ã‚·ãƒ¼ãƒ³ã‚’ã‚¹ãƒ­ãƒ¼åŒ–
            last_scene_slow = str(temp_dir / "last_scene_slow.mp4")
            subprocess.run([
                "ffmpeg", "-y", "-i", overlaid_videos[-1],
                "-filter:v", f"setpts={slowdown_factor}*PTS",
                "-c:v", "libx264", "-preset", "fast", "-crf", "18",
                "-an", last_scene_slow
            ], capture_output=True)
            overlaid_videos[-1] = last_scene_slow
        
        # 4. å‹•ç”»ã‚’çµåˆï¼ˆfilter_complexæ–¹å¼ï¼‰
        inputs = []
        for v in overlaid_videos:
            inputs.extend(["-i", v])
        
        n = len(overlaid_videos)
        filter_str = "".join([f"[{i}:v]" for i in range(n)]) + f"concat=n={n}:v=1:a=0[v]"
        
        concat_video_path = str(temp_dir / "concat.mp4")
        cmd = ["ffmpeg", "-y"] + inputs + [
            "-filter_complex", filter_str,
            "-map", "[v]",
            "-c:v", "libx264", "-preset", "fast", "-crf", "18",
            concat_video_path
        ]
        subprocess.run(cmd, capture_output=True)
        console.print("  âœ… å‹•ç”»çµåˆå®Œäº†")
        
        # 5. éŸ³å£°ã‚’è¿½åŠ 
        final_path = str(VIDEOS_DIR / f"{output_prefix}_final.mp4")
        subprocess.run([
            "ffmpeg", "-y",
            "-i", concat_video_path,
            "-i", audio_path,
            "-c:v", "copy",
            "-c:a", "aac", "-b:a", "192k",
            "-shortest",
            final_path
        ], capture_output=True)
        
        console.print(f"\n[green]ğŸ‰ å®Œæˆ: {final_path}[/green]")
        
        return final_path
    
    def run(
        self,
        article_text: str,
        headline: str,
        sub_headline: str = "",
        closing_text: str = "",
        output_prefix: Optional[str] = None,
        is_breaking: bool = True,
    ) -> NewsVideoResult:
        """ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å…¨ä½“ã‚’å®Ÿè¡Œ
        
        Args:
            article_text: è¨˜äº‹æœ¬æ–‡
            headline: ãƒ˜ãƒƒãƒ‰ãƒ©ã‚¤ãƒ³
            sub_headline: ã‚µãƒ–ãƒ˜ãƒƒãƒ‰ãƒ©ã‚¤ãƒ³
            closing_text: ç· ã‚ãƒŠãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ï¼ˆçœç•¥å¯ï¼‰
            output_prefix: å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«åãƒ—ãƒ¬ãƒ•ã‚£ãƒƒã‚¯ã‚¹
            is_breaking: BREAKING NEWSãƒãƒŠãƒ¼è¡¨ç¤º
        """
        
        if output_prefix is None:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            output_prefix = f"news_{timestamp}"
        
        console.print("\n" + "=" * 50)
        console.print(f"[bold]ğŸ“° ãƒ‹ãƒ¥ãƒ¼ã‚¹å‹•ç”»ç”Ÿæˆ: {headline[:30]}...[/bold]")
        console.print("=" * 50)
        
        try:
            # 1. è¨˜äº‹åˆ†æ
            scenes = self.analyze_article(article_text, headline)
            
            # 2. ç”»åƒç”Ÿæˆ
            scenes = self.generate_scene_images(scenes, output_prefix)
            
            # 3. å‹•ç”»ç”Ÿæˆ
            scenes = self.generate_scene_videos(scenes, output_prefix)
            
            # 4. ãƒŠãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ç”Ÿæˆï¼ˆç· ã‚å«ã‚€ï¼‰
            audio_path, audio_duration = self.generate_narration(
                scenes, output_prefix, closing_text=closing_text
            )
            
            # 5. æœ€çµ‚åˆæˆï¼ˆéŸ³å£°é•·ã«åˆã‚ã›ã¦ã‚¹ãƒ­ãƒ¼èª¿æ•´ï¼‰
            final_path = self.compose_final_video(
                scenes=scenes,
                audio_path=audio_path,
                audio_duration=audio_duration,
                headline=headline,
                sub_headline=sub_headline,
                output_prefix=output_prefix,
                is_breaking=is_breaking,
            )
            
            # å‹•ç”»ã®é•·ã•ã‚’å–å¾—
            probe = subprocess.run(
                ["ffprobe", "-v", "error", "-show_entries", "format=duration",
                 "-of", "default=noprint_wrappers=1:nokey=1", final_path],
                capture_output=True, text=True
            )
            duration = float(probe.stdout.strip())
            
            return NewsVideoResult(
                success=True,
                video_path=final_path,
                scenes=scenes,
                audio_path=audio_path,
                duration_seconds=duration,
            )
            
        except Exception as e:
            console.print(f"[red]âŒ ã‚¨ãƒ©ãƒ¼: {e}[/red]")
            import traceback
            traceback.print_exc()
            return NewsVideoResult(
                success=False,
                error_message=str(e),
            )


# CLIç”¨
if __name__ == "__main__":
    import sys
    
    pipeline = NewsVideoPipeline()
    
    # ãƒ†ã‚¹ãƒˆç”¨
    result = pipeline.run(
        article_text="""
        ã‚¹ãƒšã‚¤ãƒ³ã§è¡Œæ–¹ä¸æ˜ã«ãªã£ãŸçŒ«ãŒã€5ãƒ¶æœˆã‹ã‘ã¦250ã‚­ãƒ­ã‚’æ­©ãã€
        ãƒ•ãƒ©ãƒ³ã‚¹ã®è‡ªå®…ã«å¸°é‚„ã—ã¾ã—ãŸã€‚é£¼ã„ä¸»ã®ãƒ•ã‚¡ãƒ“ã‚¢ãƒ³ã•ã‚“ã¯ã€
        æ„›çŒ«ãƒŸãƒŒã‚·ãƒ¥ãŒæˆ»ã£ã¦ããŸæ™‚ã€ä¿¡ã˜ã‚‰ã‚Œãªã‹ã£ãŸã¨èªã£ã¦ã„ã¾ã™ã€‚
        çŒ«ã¯å°‘ã—ç—©ã›ã¦ã„ã¾ã—ãŸãŒã€å…ƒæ°—ãªæ§˜å­ã§ã—ãŸã€‚
        """,
        headline="çŒ«ãŒ250kmæ­©ã„ã¦ã‚¹ãƒšã‚¤ãƒ³ã‹ã‚‰ãƒ•ãƒ©ãƒ³ã‚¹ã®è‡ªå®…ã«å¸°é‚„",
        sub_headline="5ãƒ¶æœˆã‹ã‘ã¦155ãƒã‚¤ãƒ«ã‚’è¸ç ´",
        output_prefix="cat_journey",
    )
    
    print(f"\nçµæœ: {'æˆåŠŸ' if result.success else 'å¤±æ•—'}")
    if result.success:
        print(f"å‹•ç”»: {result.video_path}")
        print(f"é•·ã•: {result.duration_seconds:.1f}ç§’")
