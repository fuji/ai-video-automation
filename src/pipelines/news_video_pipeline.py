"""
ãƒ‹ãƒ¥ãƒ¼ã‚¹å‹•ç”»ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³

è¨˜äº‹ã‹ã‚‰è¤‡æ•°ã‚·ãƒ¼ãƒ³ã®ãƒ‹ãƒ¥ãƒ¼ã‚¹å‹•ç”»ã‚’è‡ªå‹•ç”Ÿæˆ
"""

import os
import subprocess
import json
from pathlib import Path
from dataclasses import dataclass, field
from typing import Optional
from datetime import datetime

from PIL import Image, ImageDraw, ImageFont
from rich.console import Console
from google import genai

import fal_client
import httpx
import time

from src.generators.image_generator import FluxImageGenerator
from src.generators.remotion_generator import RemotionGenerator, SceneConfig
from src.config import config, get_daily_output_dirs
from src.generators.edge_tts_generator import EdgeTTSGenerator  # ç„¡æ–™TTS
from src.editors.news_graphics import NewsGraphicsCompositor
from src.editors.intro_outro import IntroOutroGenerator, IntroOutroConfig
from src.audio.bgm_manager import BGMManager, MoodType

console = Console()


@dataclass
class Scene:
    """ã‚·ãƒ¼ãƒ³æƒ…å ±"""
    index: int
    description: str  # ã‚·ãƒ¼ãƒ³ã®èª¬æ˜
    image_prompt: str  # Fluxç”¨ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
    video_prompt: str  # Lumaç”¨ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
    subtitle: str  # ã“ã®ã‚·ãƒ¼ãƒ³ã®å­—å¹•
    image_path: Optional[str] = None
    video_path: Optional[str] = None


@dataclass
class NewsVideoResult:
    """ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³çµæœ"""
    success: bool
    video_path: Optional[str] = None
    scenes: list[Scene] = field(default_factory=list)
    audio_path: Optional[str] = None
    duration_seconds: float = 0.0
    error_message: Optional[str] = None


class NewsVideoPipeline:
    """ãƒ‹ãƒ¥ãƒ¼ã‚¹å‹•ç”»ç”Ÿæˆãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³"""
    
    def __init__(
        self,
        channel_name: str = "FJ News 24",
        num_scenes: int = 10,  # 10ã‚·ãƒ¼ãƒ³ã§ç´„60-90ç§’ã®å‹•ç”»
        scene_duration: float = 5.0,
        use_remotion: bool = True,  # Remotion ã‚’ä½¿ã†ï¼ˆç„¡æ–™ï¼‰ã‹ Luma ã‚’ä½¿ã†ï¼ˆæœ‰æ–™ï¼‰
    ):
        self.channel_name = channel_name
        self.num_scenes = num_scenes
        self.scene_duration = scene_duration
        self.use_remotion = use_remotion
        
        # æ—¥ä»˜ãƒ™ãƒ¼ã‚¹ã®å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
        self.dirs = get_daily_output_dirs()
        
        # ã‚¸ã‚§ãƒãƒ¬ãƒ¼ã‚¿ãƒ¼åˆæœŸåŒ–
        self.image_gen = FluxImageGenerator()
        self.narration_gen = EdgeTTSGenerator()  # ç„¡æ–™TTS (Edge TTS)
        self.compositor = NewsGraphicsCompositor(channel_name=channel_name)
        self.bgm_manager = BGMManager()  # BGMç®¡ç†
        self.intro_outro_gen = IntroOutroGenerator(IntroOutroConfig(
            channel_name=channel_name,
            channel_tagline="ä¸–ç•Œã®ãŠã‚‚ã—ã‚ãƒ‹ãƒ¥ãƒ¼ã‚¹",
        ))
        
        # Remotion ã‚¸ã‚§ãƒãƒ¬ãƒ¼ã‚¿ãƒ¼ï¼ˆç„¡æ–™ãƒ¢ãƒ¼ã‚·ãƒ§ãƒ³ã‚°ãƒ©ãƒ•ã‚£ãƒƒã‚¯ã‚¹ï¼‰
        if use_remotion:
            self.remotion_gen = RemotionGenerator()
            console.print(f"[cyan]ğŸ¬ Remotion ãƒ¢ãƒ¼ãƒ‰ï¼ˆç„¡æ–™ï¼‰[/cyan]")
        else:
            self.remotion_gen = None
        
        # Gemini for scene analysis
        self.gemini_client = genai.Client(api_key=config.gemini.api_key)
        
        # FAL API key for Luma (Remotionä½¿ã‚ãªã„å ´åˆ)
        if not use_remotion:
            os.environ["FAL_KEY"] = config.fal.api_key
        
        console.print(f"[green]NewsVideoPipeline initialized[/green]")
        console.print(f"  Output: {self.dirs['root']}")
        console.print(f"  Channel: {channel_name}")
        console.print(f"  Scenes: {num_scenes} x {scene_duration}s = {num_scenes * scene_duration}s")
        console.print(f"  Mode: {'Remotion (ç„¡æ–™)' if use_remotion else 'Luma (æœ‰æ–™)'}")
    
    def generate_scenes_data(
        self,
        article_text: str,
        headline: str,
        num_scenes: int = 10,
    ) -> dict:
        """è¨˜äº‹ã‹ã‚‰ã‚·ãƒ¼ãƒ³æ§‹æˆãƒ‡ãƒ¼ã‚¿ã‚’ç”Ÿæˆï¼ˆãƒ¦ãƒ¼ãƒ¢ã‚¢ä»˜ãã€é•·ã‚ãƒŠãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ï¼‰
        
        Returns:
            dict: run() ã«æ¸¡ã›ã‚‹å½¢å¼ {headline, sub_headline, scenes_data, closing_text, ...}
        """
        
        prompt = f"""ã‚ãªãŸã¯ãƒ‹ãƒ¥ãƒ¼ã‚¹å‹•ç”»ã®ã‚¹ã‚¯ãƒªãƒ—ãƒˆãƒ©ã‚¤ã‚¿ãƒ¼ã§ã™ã€‚ä»¥ä¸‹ã®è¨˜äº‹ã‚’é¢ç™½ãã€è¦–è´è€…ãŒæœ€å¾Œã¾ã§è¦‹ãŸããªã‚‹ã‚ˆã†ãªå‹•ç”»ã«æ§‹æˆã—ã¦ãã ã•ã„ã€‚

# è¨˜äº‹
ã‚¿ã‚¤ãƒˆãƒ«: {headline}
æœ¬æ–‡: {article_text}

# é‡è¦ãªæŒ‡ç¤º
1. **ãƒ¦ãƒ¼ãƒ¢ã‚¢ã‚’å…¥ã‚Œã‚‹**: çœŸé¢ç›®ã™ããšã€è»½ã„ãƒ„ãƒƒã‚³ãƒŸã‚„é¢ç™½ã„è¦–ç‚¹ã‚’å…¥ã‚Œã‚‹
2. **è¦–è´è€…ã‚’å¼•ãè¾¼ã‚€**: å†’é ­ã§ã€Œãˆã€ä½•ãã‚Œï¼Ÿã€ã¨æ€ã‚ã›ã‚‹ãƒ•ãƒƒã‚¯
3. **é•·ã‚ã®ãƒŠãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³**: å„ã‚·ãƒ¼ãƒ³8-15ç§’ç¨‹åº¦ã®ãƒŠãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ï¼ˆ30-60æ–‡å­—ï¼‰
4. **æ„Ÿæƒ…ã‚’è¾¼ã‚ã‚‹**: é©šãã€æ„Ÿå‹•ã€ç¬‘ã„ãªã©æ„Ÿæƒ…ãŒä¼ã‚ã‚‹ã‚ˆã†ã«
5. **ç· ã‚ã®ä¸€è¨€**: å°è±¡ã«æ®‹ã‚‹ç· ã‚ããã‚Š

# ãƒ¦ãƒ¼ãƒ¢ã‚¢ã®ä¾‹
âŒ çŒ«ãŒ250kmæ­©ã„ã¦å¸°é‚„ã—ã¾ã—ãŸã€‚
âœ… ã‚°ãƒ¼ã‚°ãƒ«ãƒãƒƒãƒ—ã‚‚ã€ã‚¹ãƒãƒ›ã‚‚ä½¿ã‚ãšã«250kmã€‚çŒ«ã£ã¦ã™ã”ã„ã§ã™ã­ã€‚

âŒ 2æ­³ã®å­ä¾›ãŒä¸–ç•Œè¨˜éŒ²ã‚’é”æˆã—ã¾ã—ãŸã€‚
âœ… ã¾ã ã‚ªãƒ ãƒ„ãŒå–ã‚Œã¦ãªã„ã®ã«ä¸–ç•Œè¨˜éŒ²ã€‚å¤§äººã®é¢ç›®ä¸¸ã¤ã¶ã‚Œã§ã™ã€‚

# ã‚·ãƒ¼ãƒ³æ§‹æˆï¼ˆ4ã‚°ãƒ«ãƒ¼ãƒ— Ã— 3ã‚·ãƒ¼ãƒ³ = 12ã‚·ãƒ¼ãƒ³å›ºå®šï¼‰
è¨˜äº‹ã‚’4ã¤ã®ãƒ‘ãƒ¼ãƒˆã«åˆ†å‰²ã—ã€å„ãƒ‘ãƒ¼ãƒˆã«1ã¤ã®ç”»åƒï¼ˆimage_groupï¼‰ã‚’å‰²ã‚Šå½“ã¦ã‚‹ï¼š
- ã‚°ãƒ«ãƒ¼ãƒ—1: å°å…¥ãƒ»çŠ¶æ³è¨­å®šï¼ˆ3ã‚·ãƒ¼ãƒ³ï¼‰
- ã‚°ãƒ«ãƒ¼ãƒ—2: å±•é–‹ãƒ»å‡ºæ¥äº‹ã®è©³ç´°ï¼ˆ3ã‚·ãƒ¼ãƒ³ï¼‰
- ã‚°ãƒ«ãƒ¼ãƒ—3: ã‚¯ãƒ©ã‚¤ãƒãƒƒã‚¯ã‚¹ãƒ»æœ€ã‚‚å°è±¡çš„ãªéƒ¨åˆ†ï¼ˆ3ã‚·ãƒ¼ãƒ³ï¼‰
- ã‚°ãƒ«ãƒ¼ãƒ—4: çµæœ«ãƒ»ç· ã‚ããã‚Šï¼ˆ3ã‚·ãƒ¼ãƒ³ï¼‰

å„ã‚°ãƒ«ãƒ¼ãƒ—å†…ã®3ã‚·ãƒ¼ãƒ³ã¯åŒã˜ç”»åƒã‚’ä½¿ã†ã®ã§ã€ãƒŠãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã§å¤‰åŒ–ã‚’ã¤ã‘ã‚‹ã€‚

# å‡ºåŠ›å½¢å¼ (JSON)
```json
{{
  "headline": "çŸ­ã„ã‚¿ã‚¤ãƒˆãƒ«ï¼ˆ15æ–‡å­—ä»¥å†…ï¼‰",
  "sub_headline": "ã‚µãƒ–ã‚¿ã‚¤ãƒˆãƒ«ï¼ˆ20æ–‡å­—ä»¥å†…ï¼‰",
  "mood": "emotional|funny|dramatic|informative",
  "scenes": [
    {{"image_group": 1, "visual_description": "ã‚°ãƒ«ãƒ¼ãƒ—1ã®ç”»åƒèª¬æ˜ï¼ˆè‹±èªï¼‰", "narration": "ãƒŠãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³1-1"}},
    {{"image_group": 1, "visual_description": "ï¼ˆåŒä¸Šï¼‰", "narration": "ãƒŠãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³1-2"}},
    {{"image_group": 1, "visual_description": "ï¼ˆåŒä¸Šï¼‰", "narration": "ãƒŠãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³1-3"}},
    {{"image_group": 2, "visual_description": "ã‚°ãƒ«ãƒ¼ãƒ—2ã®ç”»åƒèª¬æ˜ï¼ˆè‹±èªï¼‰", "narration": "ãƒŠãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³2-1"}},
    {{"image_group": 2, "visual_description": "ï¼ˆåŒä¸Šï¼‰", "narration": "ãƒŠãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³2-2"}},
    {{"image_group": 2, "visual_description": "ï¼ˆåŒä¸Šï¼‰", "narration": "ãƒŠãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³2-3"}},
    {{"image_group": 3, "visual_description": "ã‚°ãƒ«ãƒ¼ãƒ—3ã®ç”»åƒèª¬æ˜ï¼ˆè‹±èªï¼‰", "narration": "ãƒŠãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³3-1"}},
    {{"image_group": 3, "visual_description": "ï¼ˆåŒä¸Šï¼‰", "narration": "ãƒŠãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³3-2"}},
    {{"image_group": 3, "visual_description": "ï¼ˆåŒä¸Šï¼‰", "narration": "ãƒŠãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³3-3"}},
    {{"image_group": 4, "visual_description": "ã‚°ãƒ«ãƒ¼ãƒ—4ã®ç”»åƒèª¬æ˜ï¼ˆè‹±èªï¼‰", "narration": "ãƒŠãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³4-1"}},
    {{"image_group": 4, "visual_description": "ï¼ˆåŒä¸Šï¼‰", "narration": "ãƒŠãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³4-2"}},
    {{"image_group": 4, "visual_description": "ï¼ˆåŒä¸Šï¼‰", "narration": "ãƒŠãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³4-3"}}
  ],
  "closing_text": "ç· ã‚ã®ä¸€è¨€ï¼ˆæ—¥æœ¬èªã€20æ–‡å­—ç¨‹åº¦ï¼‰"
}}
```

**å¿…ãš12ã‚·ãƒ¼ãƒ³ï¼ˆ4ã‚°ãƒ«ãƒ¼ãƒ— Ã— 3ã‚·ãƒ¼ãƒ³ï¼‰ã§ç”Ÿæˆã—ã¦ãã ã•ã„ã€‚**"""

        console.print(f"\n[cyan]ğŸ“ ã‚·ãƒ¼ãƒ³æ§‹æˆã‚’ç”Ÿæˆä¸­ï¼ˆ{num_scenes}ã‚·ãƒ¼ãƒ³ï¼‰...[/cyan]")
        
        response = self.gemini_client.models.generate_content(
            model="gemini-2.0-flash",
            contents=prompt,
        )
        
        # JSONã‚’æŠ½å‡º
        content = response.text
        json_start = content.find("{")
        json_end = content.rfind("}") + 1
        json_str = content[json_start:json_end]
        
        try:
            data = json.loads(json_str)
        except json.JSONDecodeError as e:
            console.print(f"[yellow]âš ï¸ JSON ãƒ‘ãƒ¼ã‚¹ã‚¨ãƒ©ãƒ¼ã€ä¿®æ­£ã‚’è©¦ã¿ã¾ã™...[/yellow]")
            import re
            # ä½™åˆ†ãªã‚«ãƒ³ãƒã‚’å‰Šé™¤ã€æ”¹è¡Œã‚’æ•´ç†
            json_str = re.sub(r',\s*}', '}', json_str)
            json_str = re.sub(r',\s*]', ']', json_str)
            try:
                data = json.loads(json_str)
            except json.JSONDecodeError:
                console.print(f"[red]âŒ JSON ãƒ‘ãƒ¼ã‚¹å¤±æ•—[/red]")
                console.print(f"[dim]{content[:800]}...[/dim]")
                raise
        
        console.print(f"  âœ… {len(data.get('scenes', []))}ã‚·ãƒ¼ãƒ³ç”Ÿæˆ")
        console.print(f"  ğŸ“° {data.get('headline', headline)}")
        console.print(f"  ğŸ­ ãƒ ãƒ¼ãƒ‰: {data.get('mood', 'neutral')}")
        
        return data
    
    def analyze_article(
        self,
        article_text: str,
        headline: str,
    ) -> list[Scene]:
        """è¨˜äº‹ã‚’åˆ†æã—ã¦è¤‡æ•°ã‚·ãƒ¼ãƒ³ã«åˆ†è§£ï¼ˆå¾Œæ–¹äº’æ›ç”¨ï¼‰"""
        
        prompt = f"""ä»¥ä¸‹ã®ãƒ‹ãƒ¥ãƒ¼ã‚¹è¨˜äº‹ã‚’{self.num_scenes}ã¤ã®æ˜ åƒçš„ãªã‚·ãƒ¼ãƒ³ã«åˆ†è§£ã—ã¦ãã ã•ã„ã€‚

# è¨˜äº‹
ã‚¿ã‚¤ãƒˆãƒ«: {headline}
æœ¬æ–‡: {article_text}

# ã‚·ãƒ¼ãƒ³æ§‹æˆã‚¬ã‚¤ãƒ‰ï¼ˆ{self.num_scenes}ã‚·ãƒ¼ãƒ³ï¼‰
1. ã‚ªãƒ¼ãƒ—ãƒ‹ãƒ³ã‚°: çŠ¶æ³è¨­å®šã€ä¸»äººå…¬ã‚„èˆå°ã®ç´¹ä»‹
2. å±•é–‹1: å‡ºæ¥äº‹ã®å§‹ã¾ã‚Šã€å•é¡Œã‚„çŠ¶æ³ã®ç™ºç”Ÿ
3. å±•é–‹2: ã‚¯ãƒ©ã‚¤ãƒãƒƒã‚¯ã‚¹ã€æœ€ã‚‚å°è±¡çš„ãªç¬é–“
4. ã‚¨ãƒ³ãƒ‡ã‚£ãƒ³ã‚°: çµæœ«ã€ç¾åœ¨ã®çŠ¶æ³ã€ä½™éŸ»

# å‡ºåŠ›å½¢å¼ (JSON)
å„ã‚·ãƒ¼ãƒ³ã«ã¤ã„ã¦ä»¥ä¸‹ã‚’ç”Ÿæˆ:
- description: ã‚·ãƒ¼ãƒ³ã®èª¬æ˜ï¼ˆæ—¥æœ¬èªã€1æ–‡ã§æ˜ åƒã‚’ã‚¤ãƒ¡ãƒ¼ã‚¸ã§ãã‚‹ã‚ˆã†ã«ï¼‰
- image_prompt: Fluxç”»åƒç”Ÿæˆç”¨ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆï¼ˆè‹±èªã€70èªä»¥å†…ï¼‰
  * å…·ä½“çš„ãªè¢«å†™ä½“ã€å ´æ‰€ã€æ™‚é–“å¸¯ã€é›°å›²æ°—ã‚’å«ã‚ã‚‹
  * "photorealistic, cinematic lighting, 4K quality" ã‚’å«ã‚ã‚‹
  * äººç‰©ãŒã„ã‚‹å ´åˆã¯è¡¨æƒ…ã‚„å‹•ä½œã‚‚æå†™
- video_prompt: Lumaå‹•ç”»ç”Ÿæˆç”¨ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆï¼ˆè‹±èªã€25èªä»¥å†…ï¼‰
  * ã‚«ãƒ¡ãƒ©ãƒ¯ãƒ¼ã‚¯ï¼ˆpan, zoom, dollyç­‰ï¼‰ã‚’æŒ‡å®š
  * å‹•ãã®æ–¹å‘ã¨é€Ÿåº¦ã‚’å«ã‚ã‚‹
- subtitle: ã“ã®ã‚·ãƒ¼ãƒ³ã®å­—å¹•ï¼ˆæ—¥æœ¬èªã€20-30æ–‡å­—ã€æ„Ÿæƒ…ãŒä¼ã‚ã‚‹ã‚ˆã†ã«ï¼‰

```json
{{
  "scenes": [
    {{
      "description": "...",
      "image_prompt": "...",
      "video_prompt": "...",
      "subtitle": "..."
    }}
  ]
}}
```"""

        console.print("\n[cyan]ğŸ“ è¨˜äº‹ã‚’åˆ†æä¸­...[/cyan]")
        
        response = self.gemini_client.models.generate_content(
            model="gemini-2.0-flash",
            contents=prompt,
        )
        
        # JSONã‚’æŠ½å‡º
        content = response.text
        json_start = content.find("{")
        json_end = content.rfind("}") + 1
        json_str = content[json_start:json_end]
        
        data = json.loads(json_str)
        
        scenes = []
        for i, scene_data in enumerate(data["scenes"]):
            scene = Scene(
                index=i,
                description=scene_data["description"],
                image_prompt=scene_data["image_prompt"],
                video_prompt=scene_data["video_prompt"],
                subtitle=scene_data["subtitle"],
            )
            scenes.append(scene)
            console.print(f"  ã‚·ãƒ¼ãƒ³{i+1}: {scene.description}")
        
        return scenes
    
    def generate_scene_images(
        self,
        scenes: list[Scene],
        output_prefix: str,
    ) -> list[Scene]:
        """å„ã‚·ãƒ¼ãƒ³ã®ç”»åƒã‚’ç”Ÿæˆ"""
        
        console.print("\n[cyan]ğŸ–¼ï¸ ã‚·ãƒ¼ãƒ³ç”»åƒã‚’ç”Ÿæˆä¸­...[/cyan]")
        
        for scene in scenes:
            output_name = f"{output_prefix}_scene{scene.index + 1}"
            
            result = self.image_gen.generate(
                prompt=scene.image_prompt,
                output_name=output_name,
                image_size="portrait_16_9",  # ç¸¦å‹•ç”»ç”¨
                output_dir=self.dirs["images"],
            )
            
            if result.success:
                scene.image_path = result.file_path
                console.print(f"  âœ… ã‚·ãƒ¼ãƒ³{scene.index + 1}: {result.file_path}")
            else:
                console.print(f"  âŒ ã‚·ãƒ¼ãƒ³{scene.index + 1}: {result.error_message}")
        
        return scenes
    
    def generate_scene_videos_remotion(
        self,
        scenes: list[Scene],
        output_prefix: str,
        headline: str = "",
        sub_headline: str = "",
        is_breaking: bool = True,
        news_style: bool = True,
        mood: str = "exciting",
    ) -> list[Scene]:
        """Remotion ã§ãƒ‹ãƒ¥ãƒ¼ã‚¹é¢¨å‹•ç”»ã‚’ç”Ÿæˆ
        
        Args:
            scenes: ã‚·ãƒ¼ãƒ³ãƒªã‚¹ãƒˆï¼ˆimage_path ãŒã‚ã‚Œã°ãã‚Œã‚’èƒŒæ™¯ã«ä½¿ç”¨ï¼‰
            output_prefix: å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«åãƒ—ãƒ¬ãƒ•ã‚£ãƒƒã‚¯ã‚¹
            headline: ãƒ˜ãƒƒãƒ‰ãƒ©ã‚¤ãƒ³ï¼ˆæœ€åˆã®ã‚·ãƒ¼ãƒ³ã®ã¿è¡¨ç¤ºï¼‰
            sub_headline: ã‚µãƒ–ãƒ˜ãƒƒãƒ‰ãƒ©ã‚¤ãƒ³
            is_breaking: BREAKING NEWS è¡¨ç¤º
            news_style: ãƒ‹ãƒ¥ãƒ¼ã‚¹é¢¨ã‚¹ã‚¿ã‚¤ãƒ«ã‚’ä½¿ç”¨
            mood: ãƒ ãƒ¼ãƒ‰ï¼ˆã‚°ãƒ©ãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³èƒŒæ™¯ã®å ´åˆã«ä½¿ç”¨ï¼‰
        """
        
        console.print("\n[cyan]ğŸ¬ ã‚·ãƒ¼ãƒ³å‹•ç”»ã‚’ç”Ÿæˆä¸­ (Remotion)...[/cyan]")
        
        # ãƒ ãƒ¼ãƒ‰ã«åŸºã¥ãè‰²ï¼ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ç”¨ï¼‰
        mood_colors = {
            "exciting": ["#FF6B6B", "#FF8E53"],
            "heartwarming": ["#A8E6CF", "#DCEDC1"],
            "funny": ["#FFE66D", "#FFB347"],
            "shocking": ["#E94560", "#1A1A2E"],
            "informative": ["#4ECDC4", "#44A08D"],
        }
        
        # ç”»åƒã‚°ãƒ«ãƒ¼ãƒ—ã”ã¨ã«ã‚¢ãƒ‹ãƒ¡ãƒ¼ã‚·ãƒ§ãƒ³é€²æ—ã‚’è¨ˆç®—ï¼ˆv11æ–¹å¼ï¼‰
        # åŒã˜ç”»åƒã‚’ä½¿ã†ã‚·ãƒ¼ãƒ³ã§ã‚¢ãƒ‹ãƒ¡ãƒ¼ã‚·ãƒ§ãƒ³ãŒç¶™ç¶šã™ã‚‹
        image_groups = {}  # image_path -> list of scene indices
        image_group_numbers = {}  # image_path -> group number (1-based)
        group_counter = 1
        for scene in scenes:
            img = getattr(scene, 'image_path', None)
            if img:
                if img not in image_groups:
                    image_groups[img] = []
                    image_group_numbers[img] = group_counter
                    group_counter += 1
                image_groups[img].append(scene.index)
        
        # å„ã‚°ãƒ«ãƒ¼ãƒ—ã®åˆè¨ˆæ™‚é–“ã‚’è¨ˆç®—
        group_durations = {}
        for img, indices in image_groups.items():
            total = sum(getattr(scenes[i], 'audio_duration', 5.0) or 5.0 for i in indices)
            group_durations[img] = total
        
        # å„ã‚·ãƒ¼ãƒ³ã®ã‚¢ãƒ‹ãƒ¡ãƒ¼ã‚·ãƒ§ãƒ³é–‹å§‹/çµ‚äº†ä½ç½®ã‚’è¨ˆç®—
        group_progress = {img: 0.0 for img in image_groups}
        scene_anim = {}  # scene.index -> (start, end)
        for scene in scenes:
            img = getattr(scene, 'image_path', None)
            if img and img in group_durations:
                total = group_durations[img]
                dur = getattr(scene, 'audio_duration', 5.0) or 5.0
                start = group_progress[img] / total if total > 0 else 0
                group_progress[img] += dur
                end = group_progress[img] / total if total > 0 else 1
                scene_anim[scene.index] = (start, end)
            else:
                scene_anim[scene.index] = (0.0, 1.0)
        
        for scene in scenes:
            output_path = str(self.dirs["videos"] / f"{output_prefix}_scene{scene.index + 1}.mp4")
            duration = getattr(scene, 'audio_duration', 5.0) or 5.0
            narration_text = getattr(scene, 'narration_text', scene.subtitle) or scene.description
            anim_start, anim_end = scene_anim.get(scene.index, (0.0, 1.0))
            
            # ç”»åƒã‚°ãƒ«ãƒ¼ãƒ—ç•ªå·ã‚’å–å¾—ï¼ˆåŒã˜ç”»åƒ = åŒã˜ã‚¢ãƒ‹ãƒ¡ãƒ¼ã‚·ãƒ§ãƒ³ãƒ‘ã‚¿ãƒ¼ãƒ³ï¼‰
            img = getattr(scene, 'image_path', None)
            group_num = image_group_numbers.get(img, scene.index + 1) if img else scene.index + 1
            
            # èƒŒæ™¯ç”»åƒãŒã‚ã‚Œã°ãƒ‹ãƒ¥ãƒ¼ã‚¹é¢¨ã€ãªã‘ã‚Œã°ãƒ¢ãƒ¼ã‚·ãƒ§ãƒ³ã‚°ãƒ©ãƒ•ã‚£ãƒƒã‚¯ã‚¹
            if scene.image_path and news_style:
                # ãƒ‹ãƒ¥ãƒ¼ã‚¹é¢¨ï¼ˆèƒŒæ™¯ç”»åƒ + ã‚ªãƒ¼ãƒãƒ¼ãƒ¬ã‚¤ï¼‰
                # - ãƒãƒ£ãƒ³ãƒãƒ«ãƒ­ã‚´: å…¨ã‚·ãƒ¼ãƒ³ã§è¡¨ç¤º
                # - ãƒãƒŠãƒ¼ï¼ˆBREAKING + ã‚¿ã‚¤ãƒˆãƒ« + ã‚µãƒ–ã‚¿ã‚¤ãƒˆãƒ«ï¼‰: å…¨ã‚·ãƒ¼ãƒ³ã§è¡¨ç¤º
                # - å­—å¹•: å…¨ã‚·ãƒ¼ãƒ³ã§è¡¨ç¤º
                result = self.remotion_gen.generate_news_scene(
                    scene_number=group_num,  # ç”»åƒã‚°ãƒ«ãƒ¼ãƒ—ç•ªå·ã§ã‚¢ãƒ‹ãƒ¡ãƒ¼ã‚·ãƒ§ãƒ³ãƒ‘ã‚¿ãƒ¼ãƒ³æ±ºå®š
                    duration=duration,
                    output_path=output_path,
                    background_image=scene.image_path,
                    subtitle=narration_text if narration_text else "",  # å…¨æ–‡è¡¨ç¤º
                    headline=headline,  # å…¨ã‚·ãƒ¼ãƒ³ã§è¡¨ç¤º
                    sub_headline=sub_headline,  # å…¨ã‚·ãƒ¼ãƒ³ã§è¡¨ç¤º
                    channel_name=self.channel_name,
                    is_breaking=is_breaking,  # å…¨ã‚·ãƒ¼ãƒ³ã§è¡¨ç¤º
                    show_overlay=True,  # å…¨ã‚·ãƒ¼ãƒ³ã§è¡¨ç¤º
                    animation_start=anim_start,
                    animation_end=anim_end,
                )
            else:
                # ãƒ¢ãƒ¼ã‚·ãƒ§ãƒ³ã‚°ãƒ©ãƒ•ã‚£ãƒƒã‚¯ã‚¹é¢¨ï¼ˆã‚°ãƒ©ãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³èƒŒæ™¯ï¼‰
                base_colors = mood_colors.get(mood, mood_colors["exciting"])
                scene_config = SceneConfig(
                    scene_number=scene.index + 1,
                    duration=duration,
                    background_colors=base_colors,
                    elements=[
                        {
                            "type": "emoji",
                            "content": self._get_emoji_for_scene(scene.description),
                            "style": {"size": "xxl"},
                            "position": {"x": "center", "y": "center", "offsetY": -100},
                            "animation": {"enter": "bounce-in", "delay": 0},
                        },
                        {
                            "type": "text",
                            "content": narration_text[:40] if narration_text else scene.description[:40],
                            "style": {"size": "lg", "weight": "bold", "color": "#FFFFFF"},
                            "position": {"x": "center", "y": "center", "offsetY": 120},
                            "animation": {"enter": "fade-in-up", "delay": 0.5},
                        },
                    ],
                    subtitle=scene.subtitle[:50] if scene.subtitle else "",
                )
                result = self.remotion_gen.generate_scene(scene_config, output_path)
            
            if result.success:
                scene.video_path = output_path
                console.print(f"  âœ… ã‚·ãƒ¼ãƒ³{scene.index + 1}: {output_path} ({result.duration_seconds:.1f}ç§’)")
            else:
                console.print(f"  âŒ ã‚·ãƒ¼ãƒ³{scene.index + 1}: {result.error_message}")
        
        return scenes
    
    def _get_emoji_for_scene(self, description: str) -> str:
        """ã‚·ãƒ¼ãƒ³èª¬æ˜ã‹ã‚‰é©åˆ‡ãªçµµæ–‡å­—ã‚’é¸æŠ"""
        emoji_map = {
            "çŒ«": "ğŸ±", "çŠ¬": "ğŸ¶", "å‹•ç‰©": "ğŸ¾",
            "å®¶": "ğŸ ", "å¸°": "ğŸ ",
            "è»Š": "ğŸš—", "æ—…": "ğŸ§³", "é“": "ğŸ›£ï¸",
            "æµ·": "ğŸŒŠ", "å±±": "â›°ï¸", "ç©º": "â˜ï¸",
            "æ„›": "â¤ï¸", "å¿ƒ": "ğŸ’•",
            "é©š": "ğŸ˜±", "è¡æ’ƒ": "ğŸ’¥",
            "ç¬‘": "ğŸ˜‚", "é¢ç™½": "ğŸ¤£",
            "æ³£": "ğŸ˜­", "æ„Ÿå‹•": "ğŸ¥¹",
            "ç«": "ğŸ”¥", "ç†±": "ğŸ”¥",
            "èµ°": "ğŸƒ", "æ­©": "ğŸš¶",
            "é£Ÿ": "ğŸ½ï¸", "æ–™ç†": "ğŸ‘¨â€ğŸ³",
            "å‹": "ğŸ†", "å„ªå‹": "ğŸ¥‡",
            "ç™ºè¦‹": "ğŸ”", "èª¿æŸ»": "ğŸ”¬",
        }
        
        for keyword, emoji in emoji_map.items():
            if keyword in description:
                return emoji
        
        return "ğŸ“°"  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ
    
    def generate_scene_videos(
        self,
        scenes: list[Scene],
        output_prefix: str,
    ) -> list[Scene]:
        """å„ã‚·ãƒ¼ãƒ³ã®å‹•ç”»ã‚’ç”Ÿæˆï¼ˆLuma Dream Machine via fal.aiï¼‰"""
        
        console.print("\n[cyan]ğŸ¬ ã‚·ãƒ¼ãƒ³å‹•ç”»ã‚’ç”Ÿæˆä¸­ (Luma)...[/cyan]")
        
        for scene in scenes:
            if not scene.image_path:
                console.print(f"  âš ï¸ ã‚·ãƒ¼ãƒ³{scene.index + 1}: ç”»åƒãŒã‚ã‚Šã¾ã›ã‚“")
                continue
            
            output_path = str(self.dirs["videos"] / f"{output_prefix}_scene{scene.index + 1}.mp4")
            
            try:
                # ç”»åƒã‚’fal.aiã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
                image_url = fal_client.upload_file(scene.image_path)
                console.print(f"  ğŸ“¤ ã‚·ãƒ¼ãƒ³{scene.index + 1}: ç”»åƒã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰å®Œäº†")
                
                # Luma APIå‘¼ã³å‡ºã—
                result = fal_client.subscribe(
                    "fal-ai/luma-dream-machine/image-to-video",
                    arguments={
                        "prompt": scene.video_prompt,
                        "image_url": image_url,
                        "aspect_ratio": "9:16",
                    },
                    with_logs=False,
                )
                
                # å‹•ç”»ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
                video_url = result["video"]["url"]
                response = httpx.get(video_url)
                with open(output_path, "wb") as f:
                    f.write(response.content)
                
                scene.video_path = output_path
                console.print(f"  âœ… ã‚·ãƒ¼ãƒ³{scene.index + 1}: {output_path}")
                
            except Exception as e:
                console.print(f"  âŒ ã‚·ãƒ¼ãƒ³{scene.index + 1}: {str(e)}")
        
        return scenes
    
    def generate_narration(
        self,
        article_text: str,
        output_prefix: str,
        closing_text: str = "",
    ) -> tuple[str, float]:
        """è¨˜äº‹å…¨æ–‡ã‹ã‚‰ãƒŠãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³éŸ³å£°ã‚’ç”Ÿæˆï¼ˆç· ã‚ãƒŠãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³å«ã‚€ï¼‰
        
        Args:
            article_text: ãƒŠãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ç”¨ã®è¨˜äº‹ãƒ†ã‚­ã‚¹ãƒˆ
            output_prefix: å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«åãƒ—ãƒ¬ãƒ•ã‚£ãƒƒã‚¯ã‚¹
            closing_text: ç· ã‚ãƒŠãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ï¼ˆçœç•¥å¯ï¼‰
        
        Returns:
            tuple: (audio_path, total_duration)
        """
        
        console.print("\n[cyan]ğŸ¤ ãƒŠãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ç”Ÿæˆä¸­...[/cyan]")
        
        # è¨˜äº‹å…¨æ–‡ã‚’ãƒŠãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã«ä½¿ç”¨
        full_text = article_text
        
        main_path = str(self.dirs["audio"] / f"{output_prefix}_narration.mp3")
        result = self.narration_gen.generate(text=full_text, output_path=main_path)
        
        if not result.success:
            console.print(f"  âŒ éŸ³å£°ç”Ÿæˆå¤±æ•—: {result.error_message}")
            return None, 0
        
        console.print(f"  âœ… æœ¬ç·¨éŸ³å£°: {result.file_path} ({result.duration_seconds:.1f}ç§’)")
        
        # ç· ã‚ãƒŠãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ãŒã‚ã‚Œã°è¿½åŠ 
        if closing_text:
            console.print("  ğŸ¤ ç· ã‚ãƒŠãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ç”Ÿæˆä¸­...")
            closing_path = str(self.dirs["audio"] / f"{output_prefix}_closing.mp3")
            closing_result = self.narration_gen.generate(text=closing_text, output_path=closing_path)
            
            if closing_result.success:
                console.print(f"  âœ… ç· ã‚éŸ³å£°: {closing_result.file_path} ({closing_result.duration_seconds:.1f}ç§’)")
                
                # éŸ³å£°ã‚’çµåˆ
                combined_path = str(self.dirs["audio"] / f"{output_prefix}_full.mp3")
                subprocess.run([
                    "ffmpeg", "-y",
                    "-i", main_path, "-i", closing_path,
                    "-filter_complex", "[0:a][1:a]concat=n=2:v=0:a=1[a]",
                    "-map", "[a]", combined_path
                ], capture_output=True)
                
                # çµåˆå¾Œã®é•·ã•ã‚’å–å¾—
                probe = subprocess.run(
                    ["ffprobe", "-v", "error", "-show_entries", "format=duration",
                     "-of", "default=noprint_wrappers=1:nokey=1", combined_path],
                    capture_output=True, text=True
                )
                total_duration = float(probe.stdout.strip())
                console.print(f"  âœ… åˆè¨ˆéŸ³å£°: {total_duration:.1f}ç§’")
                return combined_path, total_duration
        
        return result.file_path, result.duration_seconds
    
    def compose_final_video(
        self,
        scenes: list[Scene],
        audio_path: str,
        audio_duration: float,
        headline: str,
        sub_headline: str,
        output_prefix: str,
        is_breaking: bool = True,
    ) -> str:
        """å…¨ã‚·ãƒ¼ãƒ³ã‚’çµåˆã—ã¦æœ€çµ‚å‹•ç”»ã‚’ä½œæˆï¼ˆéŸ³å£°é•·ã«åˆã‚ã›ã¦ã‚¹ãƒ­ãƒ¼èª¿æ•´ï¼‰"""
        
        console.print("\n[cyan]ğŸ¬ æœ€çµ‚å‹•ç”»ã‚’åˆæˆä¸­...[/cyan]")
        
        # å‹•ç”»ãŒã‚ã‚‹ã‚·ãƒ¼ãƒ³ã ã‘æŠ½å‡º
        valid_scenes = [s for s in scenes if s.video_path]
        if not valid_scenes:
            raise ValueError("æœ‰åŠ¹ãªå‹•ç”»ãŒã‚ã‚Šã¾ã›ã‚“")
        
        # æœ€åˆã®å‹•ç”»ã‹ã‚‰ã‚µã‚¤ã‚ºã‚’å–å¾—
        probe = subprocess.run(
            ["ffprobe", "-v", "error", "-select_streams", "v:0",
             "-show_entries", "stream=width,height", "-of", "csv=p=0",
             valid_scenes[0].video_path],
            capture_output=True, text=True
        )
        size_parts = [p for p in probe.stdout.strip().split(',') if p]
        width, height = int(size_parts[0]), int(size_parts[1])
        
        # ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ç”¨ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
        temp_dir = self.dirs["temp"]
        
        # 1. å„ã‚·ãƒ¼ãƒ³ã«ã‚ªãƒ¼ãƒãƒ¼ãƒ¬ã‚¤ã¨å­—å¹•ã‚’è¿½åŠ 
        overlaid_videos = []
        
        for scene in valid_scenes:
            # ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚ªãƒ¼ãƒãƒ¼ãƒ¬ã‚¤ä½œæˆ
            overlay_path = str(temp_dir / f"overlay_{scene.index}.png")
            self.compositor.create_transparent_overlay(
                width=width, height=height,
                headline=headline,
                sub_headline=sub_headline,
                is_breaking=is_breaking,
                style="solid",
                output_path=overlay_path,
            )
            
            # å­—å¹•ã‚’è¿½åŠ 
            overlay_img = Image.open(overlay_path).convert("RGBA")
            draw = ImageDraw.Draw(overlay_img)
            
            font = ImageFont.truetype(
                "/System/Library/Fonts/ãƒ’ãƒ©ã‚®ãƒè§’ã‚´ã‚·ãƒƒã‚¯ W3.ttc",
                int(height * 0.032)
            )
            
            # å­—å¹•ã‚’è¤‡æ•°è¡Œã«åˆ†å‰²ï¼ˆé•·ã„å ´åˆï¼‰
            subtitle = scene.subtitle
            if len(subtitle) > 15:
                mid = len(subtitle) // 2
                for i in range(mid, 0, -1):
                    if subtitle[i] in 'ãŒã®ã‚’ã«ã¯ã§ã¨ã€ã€‚':
                        mid = i + 1
                        break
                lines = [subtitle[:mid], subtitle[mid:]]
            else:
                lines = [subtitle]
            
            margin_x = int(width * 0.10)
            max_text_width = width - margin_x * 2
            line_height = int(height * 0.045)
            total_text_height = len(lines) * line_height
            start_y = (height - total_text_height) // 2
            
            for i, line in enumerate(lines):
                bbox = draw.textbbox((0, 0), line, font=font)
                text_w = bbox[2] - bbox[0]
                text_x = margin_x + (max_text_width - text_w) // 2
                y = start_y + i * line_height
                draw.text(
                    (text_x, y), line, font=font,
                    fill=(255, 255, 255, 255),
                    stroke_width=3,
                    stroke_fill=(0, 0, 0, 255)
                )
            
            scene_overlay_path = str(temp_dir / f"scene_overlay_{scene.index}.png")
            overlay_img.save(scene_overlay_path, "PNG")
            
            # FFmpegã§ã‚ªãƒ¼ãƒãƒ¼ãƒ¬ã‚¤åˆæˆ
            overlaid_path = str(temp_dir / f"overlaid_{scene.index}.mp4")
            subprocess.run([
                "ffmpeg", "-y",
                "-i", scene.video_path,
                "-i", scene_overlay_path,
                "-filter_complex", "[0:v][1:v]overlay=0:0",
                "-c:v", "libx264", "-preset", "fast", "-crf", "18",
                "-an", overlaid_path
            ], capture_output=True)
            
            overlaid_videos.append(overlaid_path)
            console.print(f"  âœ… ã‚·ãƒ¼ãƒ³{scene.index + 1} ã‚ªãƒ¼ãƒãƒ¼ãƒ¬ã‚¤é©ç”¨")
        
        # 2. å„ã‚·ãƒ¼ãƒ³ã®é•·ã•ã‚’å–å¾—
        def get_duration(path):
            probe = subprocess.run(
                ["ffprobe", "-v", "error", "-show_entries", "format=duration",
                 "-of", "default=noprint_wrappers=1:nokey=1", path],
                capture_output=True, text=True
            )
            return float(probe.stdout.strip())
        
        video_durations = [get_duration(v) for v in overlaid_videos]
        total_video_duration = sum(video_durations)
        
        console.print(f"  å‹•ç”»åˆè¨ˆ: {total_video_duration:.1f}ç§’, éŸ³å£°: {audio_duration:.1f}ç§’")
        
        # 3. éŸ³å£°ãŒé•·ã„å ´åˆã€æœ€å¾Œã®ã‚·ãƒ¼ãƒ³ã‚’ã‚¹ãƒ­ãƒ¼ã«ã—ã¦èª¿æ•´
        if audio_duration > total_video_duration:
            other_scenes_duration = sum(video_durations[:-1])
            needed_last_scene = audio_duration - other_scenes_duration + 0.3
            slowdown_factor = needed_last_scene / video_durations[-1]
            
            console.print(f"  æœ€å¾Œã®ã‚·ãƒ¼ãƒ³ã‚’ {slowdown_factor:.2f}x ã‚¹ãƒ­ãƒ¼ã«èª¿æ•´")
            
            # æœ€å¾Œã®ã‚·ãƒ¼ãƒ³ã‚’ã‚¹ãƒ­ãƒ¼åŒ–
            last_scene_slow = str(temp_dir / "last_scene_slow.mp4")
            subprocess.run([
                "ffmpeg", "-y", "-i", overlaid_videos[-1],
                "-filter:v", f"setpts={slowdown_factor}*PTS",
                "-c:v", "libx264", "-preset", "fast", "-crf", "18",
                "-an", last_scene_slow
            ], capture_output=True)
            overlaid_videos[-1] = last_scene_slow
        
        # 4. å‹•ç”»ã‚’çµåˆï¼ˆfilter_complexæ–¹å¼ï¼‰
        inputs = []
        for v in overlaid_videos:
            inputs.extend(["-i", v])
        
        n = len(overlaid_videos)
        filter_str = "".join([f"[{i}:v]" for i in range(n)]) + f"concat=n={n}:v=1:a=0[v]"
        
        concat_video_path = str(temp_dir / "concat.mp4")
        cmd = ["ffmpeg", "-y"] + inputs + [
            "-filter_complex", filter_str,
            "-map", "[v]",
            "-c:v", "libx264", "-preset", "fast", "-crf", "18",
            concat_video_path
        ]
        subprocess.run(cmd, capture_output=True)
        console.print("  âœ… å‹•ç”»çµåˆå®Œäº†")
        
        # 5. éŸ³å£°ã‚’è¿½åŠ 
        final_path = str(self.dirs["final"] / f"{output_prefix}_final.mp4")
        subprocess.run([
            "ffmpeg", "-y",
            "-i", concat_video_path,
            "-i", audio_path,
            "-c:v", "copy",
            "-c:a", "aac", "-b:a", "192k",
            "-shortest",
            final_path
        ], capture_output=True)
        
        console.print(f"\n[green]ğŸ‰ å®Œæˆ: {final_path}[/green]")
        
        return final_path
    
    def run(
        self,
        headline: str,
        sub_headline: str = "",
        scenes_data: list[dict] = None,
        closing_text: str = "",
        hook: str = "",  # ãƒ•ãƒƒã‚¯ï¼ˆå†’é ­ã®å¼•ãï¼‰
        keywords: list[str] = None,  # å¼·èª¿ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
        visual_style: str = "",  # æ˜ åƒã‚¹ã‚¿ã‚¤ãƒ«
        article_text: str = "",  # å¾Œæ–¹äº’æ›ç”¨
        output_prefix: Optional[str] = None,
        is_breaking: bool = True,
        existing_images: list[str] = None,  # æ—¢å­˜ç”»åƒãƒ‘ã‚¹
    ) -> NewsVideoResult:
        """ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å…¨ä½“ã‚’å®Ÿè¡Œ
        
        Args:
            headline: ãƒ˜ãƒƒãƒ‰ãƒ©ã‚¤ãƒ³
            sub_headline: ã‚µãƒ–ãƒ˜ãƒƒãƒ‰ãƒ©ã‚¤ãƒ³
            scenes_data: ã‚·ãƒ¼ãƒ³æ§‹æˆãƒ‡ãƒ¼ã‚¿ï¼ˆæ–°å½¢å¼ï¼‰
            closing_text: ç· ã‚ãƒŠãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ï¼ˆçœç•¥å¯ï¼‰
            hook: å†’é ­ã®ãƒ•ãƒƒã‚¯ï¼ˆè¦–è´è€…ã‚’å¼•ãè¾¼ã‚€ãƒ•ãƒ¬ãƒ¼ã‚ºï¼‰
            keywords: å¼·èª¿ã—ãŸã„ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒªã‚¹ãƒˆ
            visual_style: æ˜ åƒå…¨ä½“ã®ã‚¹ã‚¿ã‚¤ãƒ«ï¼ˆä¾‹: æ¸©ã‹ã¿ã®ã‚ã‚‹å®¶æ—å†™çœŸé¢¨ï¼‰
            article_text: è¨˜äº‹æœ¬æ–‡ï¼ˆå¾Œæ–¹äº’æ›ç”¨ã€scenes_dataãŒãªã„å ´åˆã«ä½¿ç”¨ï¼‰
            output_prefix: å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«åãƒ—ãƒ¬ãƒ•ã‚£ãƒƒã‚¯ã‚¹
            is_breaking: BREAKING NEWSãƒãƒŠãƒ¼è¡¨ç¤º
        """
        
        if output_prefix is None:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            output_prefix = f"news_{timestamp}"
        
        console.print("\n" + "=" * 50)
        console.print(f"[bold]ğŸ“° ãƒ‹ãƒ¥ãƒ¼ã‚¹å‹•ç”»ç”Ÿæˆ: {headline[:30]}...[/bold]")
        console.print("=" * 50)
        
        try:
            # ã‚·ãƒ¼ãƒ³æ§‹æˆãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚‹å ´åˆã¯æ–°ãƒ•ãƒ­ãƒ¼
            if scenes_data and len(scenes_data) > 0:
                return self._run_with_scene_sync(
                    headline=headline,
                    sub_headline=sub_headline,
                    scenes_data=scenes_data,
                    closing_text=closing_text,
                    hook=hook,
                    keywords=keywords or [],
                    visual_style=visual_style,
                    output_prefix=output_prefix,
                    is_breaking=is_breaking,
                    existing_images=existing_images,
                )
            
            # å¾Œæ–¹äº’æ›: å¾“æ¥ã®ãƒ•ãƒ­ãƒ¼ï¼ˆarticle_textã‹ã‚‰åˆ†æï¼‰
            if not article_text:
                return NewsVideoResult(
                    success=False,
                    error_message="scenes_data ã¾ãŸã¯ article_text ãŒå¿…è¦ã§ã™",
                )
            
            # 1. è¨˜äº‹åˆ†æ
            scenes = self.analyze_article(article_text, headline)
            
            # 2. ç”»åƒç”Ÿæˆ
            scenes = self.generate_scene_images(scenes, output_prefix)
            
            # 3. å‹•ç”»ç”Ÿæˆ
            scenes = self.generate_scene_videos(scenes, output_prefix)
            
            # 4. ãƒŠãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ç”Ÿæˆï¼ˆè¨˜äº‹å…¨æ–‡ã‚’ä½¿ç”¨ï¼‰
            audio_path, audio_duration = self.generate_narration(
                article_text, output_prefix, closing_text=closing_text
            )
            
            # 5. æœ€çµ‚åˆæˆï¼ˆéŸ³å£°é•·ã«åˆã‚ã›ã¦ã‚¹ãƒ­ãƒ¼èª¿æ•´ï¼‰
            final_path = self.compose_final_video(
                scenes=scenes,
                audio_path=audio_path,
                audio_duration=audio_duration,
                headline=headline,
                sub_headline=sub_headline,
                output_prefix=output_prefix,
                is_breaking=is_breaking,
            )
            
            # å‹•ç”»ã®é•·ã•ã‚’å–å¾—
            probe = subprocess.run(
                ["ffprobe", "-v", "error", "-show_entries", "format=duration",
                 "-of", "default=noprint_wrappers=1:nokey=1", final_path],
                capture_output=True, text=True
            )
            duration = float(probe.stdout.strip())
            
            return NewsVideoResult(
                success=True,
                video_path=final_path,
                scenes=scenes,
                audio_path=audio_path,
                duration_seconds=duration,
            )
            
        except Exception as e:
            console.print(f"[red]âŒ ã‚¨ãƒ©ãƒ¼: {e}[/red]")
            import traceback
            traceback.print_exc()
            return NewsVideoResult(
                success=False,
                error_message=str(e),
            )
    
    def _run_with_scene_sync(
        self,
        headline: str,
        sub_headline: str,
        scenes_data: list[dict],
        closing_text: str,
        hook: str,
        keywords: list[str],
        visual_style: str,
        output_prefix: str,
        is_breaking: bool,
        mood: str = "exciting",
        existing_images: list[str] = None,
    ) -> NewsVideoResult:
        """ã‚·ãƒ¼ãƒ³åŒæœŸãƒ•ãƒ­ãƒ¼: å„ã‚·ãƒ¼ãƒ³ã®ãƒŠãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã¨æ˜ åƒã‚’åŒæœŸã•ã›ã‚‹"""
        
        console.print(f"\n[cyan]ğŸ¬ ã‚·ãƒ¼ãƒ³åŒæœŸãƒ¢ãƒ¼ãƒ‰ ({len(scenes_data)}ã‚·ãƒ¼ãƒ³)[/cyan]")
        console.print(f"[cyan]ğŸ’° ãƒ¢ãƒ¼ãƒ‰: {'Remotion (ç„¡æ–™)' if self.use_remotion else 'Luma (æœ‰æ–™)'}[/cyan]")
        if hook:
            console.print(f"[yellow]ğŸ£ ãƒ•ãƒƒã‚¯: {hook}[/yellow]")
        if visual_style:
            console.print(f"[magenta]ğŸ¨ ã‚¹ã‚¿ã‚¤ãƒ«: {visual_style}[/magenta]")
        
        # 1. scenes_dataã‹ã‚‰Sceneã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’ä½œæˆ
        scenes = []
        for i, sd in enumerate(scenes_data):
            # visual_descriptionã‹ã‚‰ç”»åƒãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ç”Ÿæˆï¼ˆvisual_styleã‚’çµ±ä¸€é©ç”¨ï¼‰
            visual_desc = sd.get("visual_description", sd.get("title", ""))
            image_prompt = self._create_image_prompt(visual_desc, headline, visual_style)
            
            scene = Scene(
                index=i,
                description=visual_desc,
                image_prompt=image_prompt,
                video_prompt=f"Slow cinematic camera movement, {visual_desc}",
                subtitle=sd.get("narration", "")[:30],  # å­—å¹•ã¯çŸ­ã
            )
            # ãƒŠãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã¨å¼·èª¿ãƒ¯ãƒ¼ãƒ‰ã‚’ä¿æŒ
            scene.narration_text = sd.get("narration", "")
            scene.emphasis_word = sd.get("emphasis_word", "")
            scenes.append(scene)
            console.print(f"  ã‚·ãƒ¼ãƒ³{i+1}: {visual_desc[:40]}...")
        
        # 2. å‹•ç”»ç”Ÿæˆï¼ˆRemotion or Lumaï¼‰
        if self.use_remotion:
            # Remotion: å…ˆã«ãƒŠãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã‚’ç”Ÿæˆã—ã¦ã€ãã®é•·ã•ã«åˆã‚ã›ã‚‹
            console.print("\n[cyan]ğŸ¤ ã‚·ãƒ¼ãƒ³åˆ¥ãƒŠãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ç”Ÿæˆä¸­ï¼ˆå…ˆã«éŸ³å£°ï¼‰...[/cyan]")
            for scene in scenes:
                narration_text = getattr(scene, 'narration_text', scene.subtitle)
                if narration_text:
                    audio_path = str(self.dirs["audio"] / f"{output_prefix}_scene{scene.index + 1}.mp3")
                    result = self.narration_gen.generate(text=narration_text, output_path=audio_path)
                    if result.success:
                        scene.audio_path = audio_path
                        scene.audio_duration = result.duration_seconds
                        console.print(f"  âœ… ã‚·ãƒ¼ãƒ³{scene.index + 1}: {result.duration_seconds:.1f}ç§’")
            
            # ç”»åƒç”Ÿæˆï¼ˆãƒ‹ãƒ¥ãƒ¼ã‚¹é¢¨ã®èƒŒæ™¯ç”¨ï¼‰ã¾ãŸã¯æ—¢å­˜ç”»åƒã‚’ä½¿ç”¨
            if existing_images and len(existing_images) >= len(scenes):
                console.print("\n[cyan]ğŸ–¼ï¸ æ—¢å­˜ç”»åƒã‚’ä½¿ç”¨...[/cyan]")
                for i, scene in enumerate(scenes):
                    scene.image_path = str(Path(existing_images[i]).resolve())
                    console.print(f"  âœ… ã‚·ãƒ¼ãƒ³{i+1}: {existing_images[i]}")
            else:
                console.print("\n[cyan]ğŸ–¼ï¸ èƒŒæ™¯ç”»åƒã‚’ç”Ÿæˆä¸­ (Flux)...[/cyan]")
                scenes = self.generate_scene_images(scenes, output_prefix)
            
            # Remotion ã§å‹•ç”»ç”Ÿæˆï¼ˆèƒŒæ™¯ç”»åƒ + ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚ªãƒ¼ãƒãƒ¼ãƒ¬ã‚¤ï¼‰
            scenes = self.generate_scene_videos_remotion(
                scenes, output_prefix,
                headline=headline,
                sub_headline=sub_headline,
                is_breaking=is_breaking,
                news_style=True,
                mood=mood,
            )
        else:
            # Luma: ç”»åƒç”Ÿæˆ â†’ å‹•ç”»ç”Ÿæˆï¼ˆæœ‰æ–™ï¼‰
            scenes = self.generate_scene_images(scenes, output_prefix)
            scenes = self.generate_scene_videos(scenes, output_prefix)
        
        # 4. ã‚·ãƒ¼ãƒ³ã”ã¨ã«ãƒŠãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ç”Ÿæˆï¼ˆRemotionã®å ´åˆã¯æ—¢ã«ç”Ÿæˆæ¸ˆã¿ï¼‰
        scene_audios = []
        total_audio_duration = 0
        
        if self.use_remotion:
            # Remotion: æ—¢ã«ç”Ÿæˆæ¸ˆã¿ãªã®ã§é›†è¨ˆã®ã¿
            for scene in scenes:
                if hasattr(scene, 'audio_path') and scene.audio_path:
                    scene_audios.append(scene.audio_path)
                    total_audio_duration += getattr(scene, 'audio_duration', 0)
        else:
            # Luma: ã“ã“ã§ãƒŠãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ç”Ÿæˆ
            console.print("\n[cyan]ğŸ¤ ã‚·ãƒ¼ãƒ³åˆ¥ãƒŠãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ç”Ÿæˆä¸­...[/cyan]")
            for scene in scenes:
                narration_text = getattr(scene, 'narration_text', scene.subtitle)
                if not narration_text:
                    continue
                    
                audio_path = str(self.dirs["audio"] / f"{output_prefix}_scene{scene.index + 1}.mp3")
                result = self.narration_gen.generate(text=narration_text, output_path=audio_path)
                
                if result.success:
                    scene.audio_path = audio_path
                    scene.audio_duration = result.duration_seconds
                    total_audio_duration += result.duration_seconds
                    scene_audios.append(audio_path)
                    console.print(f"  âœ… ã‚·ãƒ¼ãƒ³{scene.index + 1}: {result.duration_seconds:.1f}ç§’")
                else:
                    console.print(f"  âŒ ã‚·ãƒ¼ãƒ³{scene.index + 1}: éŸ³å£°ç”Ÿæˆå¤±æ•—")
        
        # 5. ç· ã‚ãƒŠãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³
        if closing_text:
            closing_path = str(self.dirs["audio"] / f"{output_prefix}_closing.mp3")
            closing_result = self.narration_gen.generate(text=closing_text, output_path=closing_path)
            if closing_result.success:
                scene_audios.append(closing_path)
                total_audio_duration += closing_result.duration_seconds
                console.print(f"  âœ… ç· ã‚: {closing_result.duration_seconds:.1f}ç§’")
        
        # 6. å…¨éŸ³å£°ã‚’çµåˆ
        console.print("\n[cyan]ğŸ”Š éŸ³å£°çµåˆä¸­...[/cyan]")
        combined_audio = str(self.dirs["audio"] / f"{output_prefix}_combined.mp3")
        
        if len(scene_audios) > 1:
            # ffmpegã§çµåˆ
            concat_list = str(self.dirs["temp"] / "audio_concat.txt")
            with open(concat_list, "w") as f:
                for ap in scene_audios:
                    f.write(f"file '{ap}'\n")
            
            subprocess.run([
                "ffmpeg", "-y", "-f", "concat", "-safe", "0",
                "-i", concat_list, "-c", "copy", combined_audio
            ], capture_output=True)
        else:
            combined_audio = scene_audios[0] if scene_audios else None
        
        console.print(f"  âœ… åˆè¨ˆéŸ³å£°: {total_audio_duration:.1f}ç§’")
        
        # 6.5. BGMãƒŸãƒƒã‚¯ã‚¹
        final_audio = combined_audio
        if combined_audio:
            # ãƒŠãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰ãƒ ãƒ¼ãƒ‰ã‚’æ¤œå‡º
            all_narration = " ".join([getattr(s, 'narration_text', '') for s in scenes])
            mood = self.bgm_manager.detect_mood(headline, all_narration)
            bgm_track = self.bgm_manager.get_bgm(mood)
            
            if bgm_track and bgm_track.exists():
                console.print(f"\n[cyan]ğŸµ BGMãƒŸãƒƒã‚¯ã‚¹ä¸­... ({mood.value})[/cyan]")
                mixed_audio = str(self.dirs["audio"] / f"{output_prefix}_mixed.mp3")
                
                if self.bgm_manager.mix_audio(
                    narration_path=combined_audio,
                    bgm_path=bgm_track.path,
                    output_path=mixed_audio,
                    narration_volume=1.0,
                    bgm_volume=0.18,  # BGMã¯æ§ãˆã‚ã ãŒèã“ãˆã‚‹ç¨‹åº¦
                ):
                    final_audio = mixed_audio
                    console.print(f"  âœ… BGMè¿½åŠ : {bgm_track.name}")
                else:
                    console.print(f"  âš ï¸ BGMãƒŸãƒƒã‚¯ã‚¹å¤±æ•—ã€ãƒŠãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã®ã¿ä½¿ç”¨")
            else:
                console.print(f"  â„¹ï¸ BGMãªã—ï¼ˆ{mood.value}ç”¨BGMæœªè¨­å®šï¼‰")
        
        # 7. æœ€çµ‚åˆæˆï¼ˆã‚·ãƒ¼ãƒ³ã”ã¨ã«éŸ³å£°é•·ã«åˆã‚ã›ã‚‹ï¼‰
        # Remotion + ç”»åƒç”Ÿæˆã®å ´åˆã¯ã‚ªãƒ¼ãƒãƒ¼ãƒ¬ã‚¤ã‚’ã‚¹ã‚­ãƒƒãƒ—ï¼ˆRemotion ã§æ—¢ã«å«ã¾ã‚Œã¦ã„ã‚‹ï¼‰
        skip_overlay = self.use_remotion and any(s.image_path for s in scenes)
        
        final_path = self._compose_scene_synced_video(
            scenes=scenes,
            combined_audio=final_audio,  # BGMãƒŸãƒƒã‚¯ã‚¹æ¸ˆã¿éŸ³å£°
            total_audio_duration=total_audio_duration,
            headline=headline,
            sub_headline=sub_headline,
            output_prefix=output_prefix,
            is_breaking=is_breaking,
            skip_overlay=skip_overlay,
        )
        
        # å‹•ç”»ã®é•·ã•ã‚’å–å¾—
        probe = subprocess.run(
            ["ffprobe", "-v", "error", "-show_entries", "format=duration",
             "-of", "default=noprint_wrappers=1:nokey=1", final_path],
            capture_output=True, text=True
        )
        duration = float(probe.stdout.strip()) if probe.stdout.strip() else total_audio_duration
        
        return NewsVideoResult(
            success=True,
            video_path=final_path,
            scenes=scenes,
            audio_path=combined_audio,
            duration_seconds=duration,
        )
    
    def _create_image_prompt(self, visual_desc: str, headline: str, visual_style: str = "") -> str:
        """visual_descriptionã‹ã‚‰ç”»åƒãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ç”Ÿæˆï¼ˆã‚¹ã‚¿ã‚¤ãƒ«çµ±ä¸€ï¼‰"""
        base = "Photorealistic, cinematic lighting, 4K quality, high detail"
        
        # visual_styleãŒã‚ã‚Œã°è¿½åŠ 
        if visual_style:
            style_map = {
                "æ¸©ã‹ã¿": "warm color palette, soft lighting, heartwarming atmosphere",
                "å®¶æ—": "family-friendly, warm tones, emotional",
                "ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ã‚¿ãƒªãƒ¼": "documentary style, natural lighting, realistic",
                "ã‚³ãƒŸã‚«ãƒ«": "playful, bright colors, whimsical",
                "æ„Ÿå‹•": "emotional, touching, cinematic, dramatic lighting",
                "é©šã": "dramatic, impactful, vivid colors",
            }
            # ã‚¹ã‚¿ã‚¤ãƒ«ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’ãƒãƒƒãƒãƒ³ã‚°
            style_addition = ""
            for key, value in style_map.items():
                if key in visual_style:
                    style_addition = value
                    break
            if not style_addition:
                style_addition = visual_style  # ãã®ã¾ã¾ä½¿ç”¨
            
            return f"{base}, {style_addition}, {visual_desc}"
        
        return f"{base}, {visual_desc}"
    
    def _compose_scene_synced_video(
        self,
        scenes: list[Scene],
        combined_audio: str,
        total_audio_duration: float,
        headline: str,
        sub_headline: str,
        output_prefix: str,
        is_breaking: bool,
        skip_overlay: bool = False,
    ) -> str:
        """ã‚·ãƒ¼ãƒ³åŒæœŸã§æœ€çµ‚å‹•ç”»ã‚’åˆæˆ
        
        Args:
            skip_overlay: True ã®å ´åˆã€ã‚ªãƒ¼ãƒãƒ¼ãƒ¬ã‚¤ã‚’è¿½åŠ ã—ãªã„ï¼ˆRemotion ãƒ‹ãƒ¥ãƒ¼ã‚¹é¢¨ã®å ´åˆï¼‰
        """
        
        console.print("\n[cyan]ğŸ¬ ã‚·ãƒ¼ãƒ³åŒæœŸåˆæˆä¸­...[/cyan]")
        
        valid_scenes = [s for s in scenes if s.video_path]
        if not valid_scenes:
            raise ValueError("æœ‰åŠ¹ãªã‚·ãƒ¼ãƒ³å‹•ç”»ãŒã‚ã‚Šã¾ã›ã‚“")
        
        # å„ã‚·ãƒ¼ãƒ³ã®ç›®æ¨™æ™‚é–“ã‚’è¨ˆç®—
        num_scenes = len(valid_scenes)
        base_duration_per_scene = total_audio_duration / num_scenes
        
        console.print(f"  ã‚·ãƒ¼ãƒ³æ•°: {num_scenes}, å„ã‚·ãƒ¼ãƒ³ç›®æ¨™: {base_duration_per_scene:.1f}ç§’")
        
        # å‹•ç”»ã‚µã‚¤ã‚ºã‚’å–å¾—
        probe = subprocess.run(
            ["ffprobe", "-v", "error", "-select_streams", "v:0",
             "-show_entries", "stream=width,height", "-of", "csv=p=0",
             valid_scenes[0].video_path],
            capture_output=True, text=True
        )
        size_parts = [p for p in probe.stdout.strip().split(',') if p]
        width, height = int(size_parts[0]), int(size_parts[1])
        
        temp_dir = self.dirs["temp"]
        
        # å„ã‚·ãƒ¼ãƒ³ã‚’ç›®æ¨™æ™‚é–“ã«èª¿æ•´ã—ã¦ã‚ªãƒ¼ãƒãƒ¼ãƒ¬ã‚¤è¿½åŠ 
        adjusted_videos = []
        
        for i, scene in enumerate(valid_scenes):
            # ã‚·ãƒ¼ãƒ³åˆ¥ã®éŸ³å£°ãŒã‚ã‚Œã°ã€ãã®é•·ã•ã«åˆã‚ã›ã‚‹
            target_duration = getattr(scene, 'audio_duration', base_duration_per_scene)
            
            # å‹•ç”»ã®å®Ÿéš›ã®é•·ã•ã‚’å–å¾—
            probe = subprocess.run(
                ["ffprobe", "-v", "error", "-show_entries", "format=duration",
                 "-of", "default=noprint_wrappers=1:nokey=1", scene.video_path],
                capture_output=True, text=True
            )
            actual_duration = float(probe.stdout.strip())
            
            # ã‚¹ãƒ­ãƒ¼ç‡ã‚’è¨ˆç®—ï¼ˆæœ€å¤§2å€ã¾ã§ï¼‰
            slowdown = min(target_duration / actual_duration, 2.0)
            
            adjusted_path = str(temp_dir / f"adjusted_{i}.mp4")
            
            if skip_overlay:
                # ã‚ªãƒ¼ãƒãƒ¼ãƒ¬ã‚¤ãªã—ï¼ˆRemotion ãƒ‹ãƒ¥ãƒ¼ã‚¹é¢¨ã®å ´åˆã¯æ—¢ã«å«ã¾ã‚Œã¦ã„ã‚‹ï¼‰
                filter_complex = f"setpts={slowdown}*PTS"
                subprocess.run([
                    "ffmpeg", "-y",
                    "-i", scene.video_path,
                    "-vf", filter_complex,
                    "-t", str(target_duration),
                    "-c:v", "libx264", "-preset", "fast",
                    "-an",
                    adjusted_path
                ], capture_output=True)
            else:
                # ã‚ªãƒ¼ãƒãƒ¼ãƒ¬ã‚¤ä½œæˆï¼ˆæœ€åˆã®ã‚·ãƒ¼ãƒ³ã®ã¿ãƒ˜ãƒƒãƒ‰ãƒ©ã‚¤ãƒ³è¡¨ç¤ºï¼‰
                overlay_path = str(temp_dir / f"overlay_{i}.png")
                self.compositor.create_transparent_overlay(
                    width=width,
                    height=height,
                    headline=headline if i == 0 else "",
                    sub_headline=sub_headline if i == 0 else "",
                    is_breaking=is_breaking and i == 0,
                    output_path=overlay_path,
                    style="gradient",
                )
                
                # å‹•ç”»èª¿æ•´ï¼ˆã‚¹ãƒ­ãƒ¼ + ã‚ªãƒ¼ãƒãƒ¼ãƒ¬ã‚¤ï¼‰
                filter_complex = f"[0:v]setpts={slowdown}*PTS[slowed];[slowed][1:v]overlay=0:0"
                subprocess.run([
                    "ffmpeg", "-y",
                    "-i", scene.video_path,
                    "-i", overlay_path,
                    "-filter_complex", filter_complex,
                    "-t", str(target_duration),
                    "-c:v", "libx264", "-preset", "fast",
                    "-an",
                    adjusted_path
                ], capture_output=True)
            
            adjusted_videos.append(adjusted_path)
            console.print(f"  âœ… ã‚·ãƒ¼ãƒ³{i+1}: {actual_duration:.1f}ç§’ â†’ {target_duration:.1f}ç§’ (x{slowdown:.2f})")
        
        # ã‚¤ãƒ³ãƒˆãƒ­å‹•ç”»ã‚’ç”Ÿæˆ
        console.print("\n[cyan]ğŸ¬ ã‚¤ãƒ³ãƒˆãƒ­ç”Ÿæˆä¸­...[/cyan]")
        intro_path = str(temp_dir / "intro.mp4")
        self.intro_outro_gen.generate_intro_video(intro_path, temp_dir)
        console.print(f"  âœ… ã‚¤ãƒ³ãƒˆãƒ­: 3ç§’")
        
        # ã‚¢ã‚¦ãƒˆãƒ­å‹•ç”»ã‚’ç”Ÿæˆ
        console.print("[cyan]ğŸ¬ ã‚¢ã‚¦ãƒˆãƒ­ç”Ÿæˆä¸­...[/cyan]")
        outro_path = str(temp_dir / "outro.mp4")
        self.intro_outro_gen.generate_outro_video(outro_path, temp_dir)
        console.print(f"  âœ… ã‚¢ã‚¦ãƒˆãƒ­: 4ç§’")
        
        # å‹•ç”»ã‚’çµåˆï¼ˆã‚¤ãƒ³ãƒˆãƒ­ + ãƒ¡ã‚¤ãƒ³ + ã‚¢ã‚¦ãƒˆãƒ­ï¼‰
        console.print("\n[cyan]ğŸ¬ å…¨ä½“çµåˆä¸­...[/cyan]")
        concat_list = str(temp_dir / "video_concat.txt")
        with open(concat_list, "w") as f:
            # ã‚¤ãƒ³ãƒˆãƒ­
            if Path(intro_path).exists():
                f.write(f"file '{intro_path}'\n")
            # ãƒ¡ã‚¤ãƒ³ã‚·ãƒ¼ãƒ³
            for vp in adjusted_videos:
                f.write(f"file '{vp}'\n")
            # ã‚¢ã‚¦ãƒˆãƒ­
            if Path(outro_path).exists():
                f.write(f"file '{outro_path}'\n")
        
        concat_video = str(temp_dir / f"{output_prefix}_concat.mp4")
        subprocess.run([
            "ffmpeg", "-y", "-f", "concat", "-safe", "0",
            "-i", concat_list, "-c", "copy", concat_video
        ], capture_output=True)
        
        # éŸ³å£°ã‚’è¿½åŠ ï¼ˆã‚¤ãƒ³ãƒˆãƒ­åˆ†ã¯ç„¡éŸ³ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°ï¼‰
        final_path = str(self.dirs["final"] / f"{output_prefix}_final.mp4")
        
        if combined_audio:
            # ã‚¤ãƒ³ãƒˆãƒ­åˆ†ã®ç„¡éŸ³(3ç§’)ã‚’éŸ³å£°ã®å‰ã«ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°
            padded_audio = str(temp_dir / f"{output_prefix}_padded.mp3")
            pad_result = subprocess.run([
                "ffmpeg", "-y",
                "-f", "lavfi", "-t", "3", "-i", "anullsrc=r=44100:cl=stereo",  # 3ç§’ç„¡éŸ³
                "-i", combined_audio,
                "-filter_complex", "[0:a][1:a]concat=n=2:v=0:a=1[out]",
                "-map", "[out]",
                "-c:a", "libmp3lame", "-b:a", "192k",
                padded_audio
            ], capture_output=True, text=True)
            
            if pad_result.returncode != 0:
                console.print(f"[yellow]âš ï¸ ç„¡éŸ³ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°å¤±æ•—ã€å…ƒã®éŸ³å£°ã‚’ä½¿ç”¨[/yellow]")
                padded_audio = combined_audio
            
            # ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°æ¸ˆã¿éŸ³å£°ã‚’å‹•ç”»ã¨åˆæˆ
            result = subprocess.run([
                "ffmpeg", "-y",
                "-i", concat_video,
                "-i", padded_audio,
                "-c:v", "copy",
                "-c:a", "aac", "-b:a", "192k",
                "-shortest",
                final_path
            ], capture_output=True, text=True)
            
            if result.returncode != 0:
                console.print(f"[red]âš ï¸ éŸ³å£°è¿½åŠ ã‚¨ãƒ©ãƒ¼: {result.stderr[:200]}[/red]")
        else:
            subprocess.run(["cp", concat_video, final_path])
        
        console.print(f"\n[green]ğŸ‰ å®Œæˆ: {final_path}[/green]")
        
        return final_path


# CLIç”¨
if __name__ == "__main__":
    import sys
    
    pipeline = NewsVideoPipeline()
    
    # ãƒ†ã‚¹ãƒˆç”¨
    result = pipeline.run(
        article_text="""
        ã‚¹ãƒšã‚¤ãƒ³ã§è¡Œæ–¹ä¸æ˜ã«ãªã£ãŸçŒ«ãŒã€5ãƒ¶æœˆã‹ã‘ã¦250ã‚­ãƒ­ã‚’æ­©ãã€
        ãƒ•ãƒ©ãƒ³ã‚¹ã®è‡ªå®…ã«å¸°é‚„ã—ã¾ã—ãŸã€‚é£¼ã„ä¸»ã®ãƒ•ã‚¡ãƒ“ã‚¢ãƒ³ã•ã‚“ã¯ã€
        æ„›çŒ«ãƒŸãƒŒã‚·ãƒ¥ãŒæˆ»ã£ã¦ããŸæ™‚ã€ä¿¡ã˜ã‚‰ã‚Œãªã‹ã£ãŸã¨èªã£ã¦ã„ã¾ã™ã€‚
        çŒ«ã¯å°‘ã—ç—©ã›ã¦ã„ã¾ã—ãŸãŒã€å…ƒæ°—ãªæ§˜å­ã§ã—ãŸã€‚
        """,
        headline="çŒ«ãŒ250kmæ­©ã„ã¦ã‚¹ãƒšã‚¤ãƒ³ã‹ã‚‰ãƒ•ãƒ©ãƒ³ã‚¹ã®è‡ªå®…ã«å¸°é‚„",
        sub_headline="5ãƒ¶æœˆã‹ã‘ã¦155ãƒã‚¤ãƒ«ã‚’è¸ç ´",
        output_prefix="cat_journey",
    )
    
    print(f"\nçµæœ: {'æˆåŠŸ' if result.success else 'å¤±æ•—'}")
    if result.success:
        print(f"å‹•ç”»: {result.video_path}")
        print(f"é•·ã•: {result.duration_seconds:.1f}ç§’")
