"""
æµ·å¤–ãŠã‚‚ã—ã‚ãƒ‹ãƒ¥ãƒ¼ã‚¹å–å¾—ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«

UPI Odd News ãªã©ã‹ã‚‰é¢ç™½ã„ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’å–å¾—ã—ã¦ã‚¹ã‚³ã‚¢ãƒªãƒ³ã‚°
"""

import feedparser
import httpx
from dataclasses import dataclass
from typing import Optional
from datetime import datetime
import re
import json

from bs4 import BeautifulSoup
from rich.console import Console

console = Console()


@dataclass
class NewsArticle:
    """ãƒ‹ãƒ¥ãƒ¼ã‚¹è¨˜äº‹"""
    title: str
    url: str
    summary: str
    published: Optional[datetime] = None
    source: str = ""
    score: int = 0
    full_text: Optional[str] = None
    
    def to_dict(self) -> dict:
        return {
            "title": self.title,
            "url": self.url,
            "summary": self.summary,
            "published": self.published.isoformat() if self.published else None,
            "source": self.source,
            "score": self.score,
        }


class OddNewsFetcher:
    """æµ·å¤–ãŠã‚‚ã—ã‚ãƒ‹ãƒ¥ãƒ¼ã‚¹å–å¾—"""
    
    # RSSãƒ•ã‚£ãƒ¼ãƒ‰
    RSS_FEEDS = {
        "upi_odd": "https://rss.upi.com/news/odd_news.rss",
        "reddit_nottheonion": "https://www.reddit.com/r/nottheonion/.rss",
        "reddit_upliftingnews": "https://www.reddit.com/r/UpliftingNews/.rss",
        "reddit_mademesmile": "https://www.reddit.com/r/MadeMeSmile/.rss",
        "bbc_news": "https://feeds.bbci.co.uk/news/rss.xml",
    }
    
    # ã‚¹ã‚³ã‚¢ãƒªãƒ³ã‚°ç”¨ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
    SCORE_KEYWORDS = {
        # å‹•ç‰©ç³» (é«˜ã‚¹ã‚³ã‚¢)
        "cat": 15, "dog": 15, "çŒ«": 15, "çŠ¬": 15,
        "animal": 10, "pet": 10, "bird": 10,
        "fox": 12, "bear": 12, "elephant": 12,
        
        # å¥‡è·¡ãƒ»æ„Ÿå‹•ç³»
        "miracle": 15, "rescue": 12, "save": 10,
        "reunite": 15, "found": 10, "return": 10,
        "survive": 12, "incredible": 10,
        
        # é¢ç™½ã„ç³»
        "funny": 8, "bizarre": 8, "unusual": 8,
        "weird": 8, "strange": 8,
        
        # ä¸–ç•Œè¨˜éŒ²ç³»
        "world record": 15, "guinness": 15,
        "first": 10, "largest": 10, "oldest": 10,
        
        # ãƒã‚¬ãƒ†ã‚£ãƒ– (æ¸›ç‚¹)
        "death": -20, "die": -15, "kill": -20,
        "accident": -10, "crash": -10,
        "arrest": -15, "crime": -15,
    }
    
    def __init__(self):
        self.client = httpx.Client(
            timeout=30.0,
            headers={
                "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36"
            },
            follow_redirects=True,
        )
    
    def fetch_rss(self, feed_url: str, source_name: str) -> list[NewsArticle]:
        """RSSãƒ•ã‚£ãƒ¼ãƒ‰ã‹ã‚‰è¨˜äº‹ã‚’å–å¾—"""
        articles = []
        
        try:
            # SSLå•é¡Œã‚’å›é¿ã™ã‚‹ãŸã‚httpxã§å–å¾—ã—ã¦ã‹ã‚‰ãƒ‘ãƒ¼ã‚¹
            response = self.client.get(feed_url)
            feed = feedparser.parse(response.text)
            
            for entry in feed.entries[:20]:  # æœ€æ–°20ä»¶
                # å…¬é–‹æ—¥æ™‚
                published = None
                if hasattr(entry, 'published_parsed') and entry.published_parsed:
                    published = datetime(*entry.published_parsed[:6])
                
                # ã‚µãƒãƒªãƒ¼
                summary = ""
                if hasattr(entry, 'summary'):
                    summary = BeautifulSoup(entry.summary, 'html.parser').get_text()[:200]
                
                article = NewsArticle(
                    title=entry.title,
                    url=entry.link,
                    summary=summary,
                    published=published,
                    source=source_name,
                )
                articles.append(article)
                
        except Exception as e:
            console.print(f"[red]RSSå–å¾—ã‚¨ãƒ©ãƒ¼ ({source_name}): {e}[/red]")
        
        return articles
    
    def score_article(self, article: NewsArticle) -> int:
        """è¨˜äº‹ã‚’ã‚¹ã‚³ã‚¢ãƒªãƒ³ã‚°"""
        score = 50  # ãƒ™ãƒ¼ã‚¹ã‚¹ã‚³ã‚¢
        
        text = f"{article.title} {article.summary}".lower()
        
        for keyword, points in self.SCORE_KEYWORDS.items():
            if keyword.lower() in text:
                score += points
        
        # ã‚¿ã‚¤ãƒˆãƒ«ã®é•·ã• (çŸ­ã™ããƒ»é•·ã™ãã¯æ¸›ç‚¹)
        title_len = len(article.title)
        if title_len < 20:
            score -= 10
        elif title_len > 100:
            score -= 5
        
        # ã‚¹ã‚³ã‚¢ã‚’0-100ã«æ­£è¦åŒ–
        score = max(0, min(100, score))
        
        return score
    
    def fetch_full_article(self, url: str) -> Optional[str]:
        """è¨˜äº‹æœ¬æ–‡ã‚’å–å¾—"""
        try:
            response = self.client.get(url)
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # UPI ã®è¨˜äº‹æœ¬æ–‡ã‚’å–å¾—
            article_body = soup.find('article') or soup.find('div', class_='article-body')
            
            if article_body:
                paragraphs = article_body.find_all('p')
                text = '\n'.join([p.get_text() for p in paragraphs])
                return text[:2000]  # æœ€å¤§2000æ–‡å­—
            
            return None
            
        except Exception as e:
            console.print(f"[red]è¨˜äº‹å–å¾—ã‚¨ãƒ©ãƒ¼: {e}[/red]")
            return None
    
    def fetch_top_news(self, limit: int = 5) -> list[NewsArticle]:
        """ä¸Šä½Nä»¶ã®ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’å–å¾—"""
        
        console.print("[cyan]ğŸ“° ãƒ‹ãƒ¥ãƒ¼ã‚¹å–å¾—ä¸­...[/cyan]")
        
        all_articles = []
        
        # å…¨RSSãƒ•ã‚£ãƒ¼ãƒ‰ã‹ã‚‰å–å¾—
        for source_name, feed_url in self.RSS_FEEDS.items():
            articles = self.fetch_rss(feed_url, source_name)
            all_articles.extend(articles)
            console.print(f"  {source_name}: {len(articles)}ä»¶")
        
        # ã‚¹ã‚³ã‚¢ãƒªãƒ³ã‚°
        console.print("[cyan]ğŸ“Š ã‚¹ã‚³ã‚¢ãƒªãƒ³ã‚°ä¸­...[/cyan]")
        for article in all_articles:
            article.score = self.score_article(article)
        
        # ã‚¹ã‚³ã‚¢é †ã«ã‚½ãƒ¼ãƒˆ
        all_articles.sort(key=lambda x: x.score, reverse=True)
        
        # ä¸Šä½Nä»¶ã‚’è¿”ã™
        top_articles = all_articles[:limit]
        
        console.print(f"[green]âœ… ä¸Šä½{len(top_articles)}ä»¶ã‚’é¸å‡º[/green]")
        
        return top_articles
    
    def format_for_discord(self, articles: list[NewsArticle]) -> str:
        """Discordç”¨ã«ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ"""
        
        lines = ["ğŸ“° **ä»Šæ—¥ã®ãŠã‚‚ã—ã‚ãƒ‹ãƒ¥ãƒ¼ã‚¹å€™è£œ:**\n"]
        
        emojis = ["1ï¸âƒ£", "2ï¸âƒ£", "3ï¸âƒ£", "4ï¸âƒ£", "5ï¸âƒ£"]
        
        for i, article in enumerate(articles):
            emoji = emojis[i] if i < len(emojis) else f"{i+1}."
            
            # ã‚¿ã‚¤ãƒˆãƒ«ã‚’æ—¥æœ¬èªã«ç°¡æ˜“ç¿»è¨³ï¼ˆå¾Œã§AIç¿»è¨³ã«ç½®ãæ›ãˆï¼‰
            title = article.title[:50]
            
            lines.append(f"{emoji} **{title}** (ã‚¹ã‚³ã‚¢: {article.score})")
            lines.append(f"   {article.summary[:80]}...")
            lines.append("")
        
        lines.append("ç•ªå·ã§é¸æŠ / ã€Œã‚¹ã‚­ãƒƒãƒ—ã€ã§ä»Šæ—¥ã¯ãƒ‘ã‚¹")
        
        return "\n".join(lines)


# CLIç”¨
if __name__ == "__main__":
    fetcher = OddNewsFetcher()
    articles = fetcher.fetch_top_news(5)
    
    print("\n" + "=" * 50)
    for i, article in enumerate(articles, 1):
        print(f"\n{i}. [{article.score}ç‚¹] {article.title}")
        print(f"   {article.url}")
        print(f"   {article.summary[:100]}...")
    
    print("\n" + fetcher.format_for_discord(articles))
